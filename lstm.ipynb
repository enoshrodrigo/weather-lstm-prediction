{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0eb5154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /home/codespace/.local/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (1.15.3)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, astunparse, rich, keras, tensorflow\n",
      "\u001b[2K  Attempting uninstall: numpy━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/26\u001b[0m [protobuf]nsorboard-data-server]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/26\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/26\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.62;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/26\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [tensorflow]0m \u001b[32m25/26\u001b[0m [tensorflow]s]wn-it-py]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages in Jupyter Notebook\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow scipy requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cf04fe-d606-48b7-bea1-ec39e1acdabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.ipynb  lstm.py  read.md  weather_data.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdfe8d2f-ae31-4bef-9332-9b198bbf0c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU.\n",
      "Filtered data for Sri Lanka: 147480 rows (from 147480 total)\n",
      "Dataset shape: (147480, 24)\n",
      "\n",
      "First 5 rows:\n",
      "       time  weathercode  temperature_2m_max  temperature_2m_min  \\\n",
      "0  1/1/2010            2                30.0                22.7   \n",
      "1  1/2/2010           51                29.9                23.5   \n",
      "2  1/3/2010           51                29.5                23.2   \n",
      "3  1/4/2010            2                28.9                21.9   \n",
      "4  1/5/2010            1                28.1                21.3   \n",
      "\n",
      "   temperature_2m_mean  apparent_temperature_max  apparent_temperature_min  \\\n",
      "0                 26.1                      34.4                      25.2   \n",
      "1                 26.2                      33.8                      26.2   \n",
      "2                 26.0                      34.3                      26.3   \n",
      "3                 25.3                      31.6                      23.4   \n",
      "4                 24.5                      30.1                      23.1   \n",
      "\n",
      "   apparent_temperature_mean     sunrise      sunset  ...  \\\n",
      "0                       29.2  6:22:00 AM  6:05:00 PM  ...   \n",
      "1                       29.8  6:22:00 AM  6:06:00 PM  ...   \n",
      "2                       29.9  6:23:00 AM  6:06:00 PM  ...   \n",
      "3                       27.8  6:23:00 AM  6:07:00 PM  ...   \n",
      "4                       26.1  6:23:00 AM  6:07:00 PM  ...   \n",
      "\n",
      "   precipitation_hours  windspeed_10m_max  windgusts_10m_max  \\\n",
      "0                    0               11.7               27.4   \n",
      "1                    1               13.0               27.0   \n",
      "2                    3               12.3               27.4   \n",
      "3                    0               17.0               34.6   \n",
      "4                    0               18.7               37.1   \n",
      "\n",
      "   winddirection_10m_dominant  et0_fao_evapotranspiration  latitude  \\\n",
      "0                          20                        4.58       7.0   \n",
      "1                          24                        3.84       7.0   \n",
      "2                          16                        3.65       7.0   \n",
      "3                         356                        3.79       7.0   \n",
      "4                         355                        4.97       7.0   \n",
      "\n",
      "   longitude  elevation    country     city  \n",
      "0  79.899994         16  Sri Lanka  Colombo  \n",
      "1  79.899994         16  Sri Lanka  Colombo  \n",
      "2  79.899994         16  Sri Lanka  Colombo  \n",
      "3  79.899994         16  Sri Lanka  Colombo  \n",
      "4  79.899994         16  Sri Lanka  Colombo  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Missing values:\n",
      "time                          0\n",
      "weathercode                   0\n",
      "temperature_2m_max            0\n",
      "temperature_2m_min            0\n",
      "temperature_2m_mean           0\n",
      "apparent_temperature_max      0\n",
      "apparent_temperature_min      0\n",
      "apparent_temperature_mean     0\n",
      "sunrise                       0\n",
      "sunset                        0\n",
      "shortwave_radiation_sum       0\n",
      "precipitation_sum             0\n",
      "rain_sum                      0\n",
      "snowfall_sum                  0\n",
      "precipitation_hours           0\n",
      "windspeed_10m_max             0\n",
      "windgusts_10m_max             0\n",
      "winddirection_10m_dominant    0\n",
      "et0_fao_evapotranspiration    0\n",
      "latitude                      0\n",
      "longitude                     0\n",
      "elevation                     0\n",
      "country                       0\n",
      "city                          0\n",
      "dtype: int64\n",
      "Applied sinh-arcsinh transformation to precipitation values\n",
      "\n",
      "Precipitation-related columns found: ['precipitation_sum', 'rain_sum', 'precipitation_hours', 'windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant', 'precipitation_sum_original', 'precipitation_sum_transformed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change'] = df[col].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change'] = df[col].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change'] = df[col].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change'] = df[col].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change'] = df[col].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
      "/tmp/ipykernel_32510/2803191749.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
      "/tmp/ipykernel_32510/2803191749.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change'] = df[col].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
      "/tmp/ipykernel_32510/2803191749.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
      "/tmp/ipykernel_32510/2803191749.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['light_rain'] = ((df['precipitation_sum'] > 0.1) & (df['precipitation_sum'] <= 2.5)).astype(int)\n",
      "/tmp/ipykernel_32510/2803191749.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['moderate_rain'] = ((df['precipitation_sum'] > 2.5) & (df['precipitation_sum'] <= 10)).astype(int)\n",
      "/tmp/ipykernel_32510/2803191749.py:196: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['heavy_rain'] = ((df['precipitation_sum'] > 10) & (df['precipitation_sum'] <= 50)).astype(int)\n",
      "/tmp/ipykernel_32510/2803191749.py:197: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['extreme_rain'] = (df['precipitation_sum'] > 50).astype(int)\n",
      "/tmp/ipykernel_32510/2803191749.py:210: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rain_streak'] = rain_streaks\n",
      "/tmp/ipykernel_32510/2803191749.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
      "/tmp/ipykernel_32510/2803191749.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
      "/tmp/ipykernel_32510/2803191749.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
      "/tmp/ipykernel_32510/2803191749.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
      "/tmp/ipykernel_32510/2803191749.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
      "/tmp/ipykernel_32510/2803191749.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
      "/tmp/ipykernel_32510/2803191749.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
      "/tmp/ipykernel_32510/2803191749.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
      "/tmp/ipykernel_32510/2803191749.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['weather_change'] = (df['weathercode'].diff() != 0).astype(int)\n",
      "/tmp/ipykernel_32510/2803191749.py:229: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['dominant_weathercode_7d'] = df['weathercode'].rolling(window=7).apply(\n",
      "/tmp/ipykernel_32510/2803191749.py:233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['weather_anomaly'] = (df['weathercode'] != df['dominant_weathercode_7d']).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded city into 30 categories\n",
      "Encoded country into 1 categories\n",
      "Removing temperature_2m_mean as it's derived from max and min\n",
      "Removing apparent_temperature_mean as it's derived from max and min\n",
      "Removing 1 snow-related features as they're irrelevant for Sri Lanka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32510/2803191749.py:275: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  date_range = pd.date_range(start=start_date, end=end_date, freq='M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ENSO index data\n",
      "\n",
      "Target columns: ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum_original']\n",
      "Binary precipitation targets: ['light_rain', 'moderate_rain', 'heavy_rain', 'extreme_rain']\n",
      "\n",
      "Selected feature columns (365):\n",
      "['weathercode', 'apparent_temperature_max', 'apparent_temperature_min', 'shortwave_radiation_sum', 'precipitation_sum', 'rain_sum', 'precipitation_hours', 'windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'] ... and more\n",
      "\n",
      "Precipitation Summary Statistics:\n",
      "count    147480.000000\n",
      "mean          5.975637\n",
      "std          10.215294\n",
      "min           0.000000\n",
      "25%           0.400000\n",
      "50%           2.600000\n",
      "75%           7.500000\n",
      "max         338.800000\n",
      "Name: precipitation_sum_original, dtype: float64\n",
      "Days with light rain: 43453 (29.5%)\n",
      "Days with moderate rain: 46666 (31.6%)\n",
      "Days with heavy rain: 25960 (17.6%)\n",
      "Days with extreme rain: 1184 (0.8%)\n",
      "\n",
      "Train set: 117984 rows (from 2010-01-01 00:00:00 to 2020-10-07 00:00:00)\n",
      "Validation set: 14748 rows (from 2020-10-07 00:00:00 to 2022-02-11 00:00:00)\n",
      "Test set: 14748 rows (from 2022-02-11 00:00:00 to 2023-06-17 00:00:00)\n",
      "Test set includes 6030 SW monsoon days and 5988 NE monsoon days\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.2 GiB for an array with shape (117920, 60, 365) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 414\u001b[39m\n\u001b[32m    411\u001b[39m prediction_length = \u001b[32m5\u001b[39m  \u001b[38;5;66;03m# Predict 5 days ahead\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Create sequences with binary targets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m X_train, y_train, binary_y_train = \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_features_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_binary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m X_val, y_val, binary_y_val = create_sequences(\n\u001b[32m    419\u001b[39m     val_features_scaled, val_targets_scaled, val_binary,\n\u001b[32m    420\u001b[39m     seq_length=sequence_length, pred_length=prediction_length)\n\u001b[32m    422\u001b[39m X_test, y_test, binary_y_test = create_sequences(\n\u001b[32m    423\u001b[39m     test_features_scaled, test_targets_scaled, test_binary,\n\u001b[32m    424\u001b[39m     seq_length=sequence_length, pred_length=prediction_length)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 405\u001b[39m, in \u001b[36mcreate_sequences\u001b[39m\u001b[34m(features, targets, binary_targets, seq_length, pred_length)\u001b[39m\n\u001b[32m    402\u001b[39m         binary_y.append(binary_targets[i+seq_length:i+seq_length+pred_length])\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m binary_targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, np.array(y), np.array(binary_y)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(X), np.array(y)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 19.2 GiB for an array with shape (117920, 60, 365) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Input\n",
    "from tensorflow.keras.layers import Conv1D, TimeDistributed, MaxPooling1D, Flatten, Add, Attention, Reshape\n",
    "from tensorflow.keras.layers import ConvLSTM2D, RepeatVector, Concatenate, Lambda, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, accuracy_score\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU.\")\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    # For Google Colab\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    df = pd.read_csv(filename)\n",
    "except ImportError:\n",
    "    # For local execution\n",
    "    df = pd.read_csv('weather_data.csv')  # Update with your file path\n",
    "\n",
    "# Filter for Sri Lanka data\n",
    "if 'country' in df.columns:\n",
    "    original_len = len(df)\n",
    "    df = df[df['country'] == 'Sri Lanka']\n",
    "    print(f\"Filtered data for Sri Lanka: {len(df)} rows (from {original_len} total)\")\n",
    "    if len(df) == 0:\n",
    "        print(\"WARNING: No Sri Lanka data found. Using all available data.\")\n",
    "        df = pd.read_csv('weather_data.csv')  # Reload the data\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Convert time to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# IMPROVEMENT: Advanced data preprocessing using sinh-arcsinh transformation\n",
    "def sinh_arcsinh_transform(x, epsilon=0.01, delta=1.0):\n",
    "    \"\"\"Apply sinh-arcsinh transformation to better handle extreme values\"\"\"\n",
    "    return np.sinh(delta * np.arcsinh(x) - epsilon)\n",
    "\n",
    "def inverse_sinh_arcsinh_transform(x, epsilon=0.01, delta=1.0):\n",
    "    \"\"\"Inverse transform to return to original scale\"\"\"\n",
    "    return np.sinh((np.arcsinh(x) + epsilon) / delta)\n",
    "\n",
    "# Apply transformation to precipitation values if they exist\n",
    "if 'precipitation_sum' in df.columns:\n",
    "    # Store original values for reference\n",
    "    df['precipitation_sum_original'] = df['precipitation_sum'].copy()\n",
    "\n",
    "    # Handle zeros and small values carefully\n",
    "    mask = df['precipitation_sum'] > 0\n",
    "    df.loc[mask, 'precipitation_sum_transformed'] = sinh_arcsinh_transform(df.loc[mask, 'precipitation_sum'])\n",
    "    df.loc[~mask, 'precipitation_sum_transformed'] = 0\n",
    "\n",
    "    print(f\"Applied sinh-arcsinh transformation to precipitation values\")\n",
    "\n",
    "    # Use transformed values for modeling\n",
    "    # Note: We'll keep the original column name for simplicity in the code\n",
    "    df['precipitation_sum'] = df['precipitation_sum_transformed']\n",
    "\n",
    "# Standard time features - Moved this section up\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['dayofyear'] = df['time'].dt.dayofyear\n",
    "df['year'] = df['time'].dt.year\n",
    "df['season'] = (df['month'] % 12 + 3) // 3  # 1: spring, 2: summer, 3: fall, 4: winter\n",
    "df['week_of_year'] = df['time'].dt.isocalendar().week\n",
    "\n",
    "# IMPROVEMENT: Sri Lanka-specific feature engineering\n",
    "# 1. Add monsoon season indicators\n",
    "df['southwest_monsoon'] = ((df['month'] >= 5) & (df['month'] <= 9)).astype(int)\n",
    "df['northeast_monsoon'] = ((df['month'] >= 11) | (df['month'] <= 3)).astype(int)\n",
    "df['inter_monsoon1'] = (df['month'] == 4).astype(int)\n",
    "df['inter_monsoon2'] = (df['month'] == 10).astype(int)\n",
    "\n",
    "# 2. Add typical monsoon intensity based on historical patterns\n",
    "# This is a simplified model - ideally would be based on historical data\n",
    "monsoon_intensity = {\n",
    "    1: 0.7,  # January - NE monsoon\n",
    "    2: 0.4,  # February - NE monsoon (weakening)\n",
    "    3: 0.2,  # March - end of NE monsoon\n",
    "    4: 0.3,  # April - inter-monsoon\n",
    "    5: 0.6,  # May - start of SW monsoon\n",
    "    6: 0.8,  # June - SW monsoon\n",
    "    7: 0.9,  # July - SW monsoon peak\n",
    "    8: 0.9,  # August - SW monsoon peak\n",
    "    9: 0.7,  # September - SW monsoon (weakening)\n",
    "    10: 0.5, # October - inter-monsoon\n",
    "    11: 0.6, # November - start of NE monsoon\n",
    "    12: 0.8, # December - NE monsoon\n",
    "}\n",
    "\n",
    "df['monsoon_intensity'] = df['month'].map(monsoon_intensity)\n",
    "\n",
    "# 3. Regional indicators for Sri Lanka's climate zones\n",
    "# If city information is available\n",
    "if 'city' in df.columns:\n",
    "    # Sri Lanka climate zone mapping (simplified)\n",
    "    wet_zone_cities = ['Colombo', 'Galle', 'Ratnapura', 'Kalutara']\n",
    "    dry_zone_cities = ['Anuradhapura', 'Hambantota', 'Jaffna', 'Mannar']\n",
    "    intermediate_zone_cities = ['Kandy', 'Badulla', 'Kurunegala']\n",
    "\n",
    "    # Create climate zone indicators\n",
    "    df['wet_zone'] = df['city'].isin(wet_zone_cities).astype(int)\n",
    "    df['dry_zone'] = df['city'].isin(dry_zone_cities).astype(int)\n",
    "    df['intermediate_zone'] = df['city'].isin(intermediate_zone_cities).astype(int)\n",
    "\n",
    "\n",
    "# Add cyclical encoding for seasonal patterns\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['dayofyear']/365)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['dayofyear']/365)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * 12/24)  # Assume noon for daily data\n",
    "df['hour_cos'] = np.cos(2 * np.pi * 12/24)\n",
    "\n",
    "# Handle time columns: sunrise and sunset\n",
    "def time_to_minutes(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        if isinstance(time_str, str) and ('AM' in time_str or 'PM' in time_str):\n",
    "            time_obj = pd.to_datetime(time_str, format='%I:%M:%S %p').time()\n",
    "        else:\n",
    "            time_obj = pd.to_datetime(time_str).time()\n",
    "        return time_obj.hour * 60 + time_obj.minute\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "if 'sunrise' in df.columns:\n",
    "    df['sunrise_minutes'] = df['sunrise'].apply(time_to_minutes)\n",
    "    df['sunset_minutes'] = df['sunset'].apply(time_to_minutes)\n",
    "    df['daylight_minutes'] = df['sunset_minutes'] - df['sunrise_minutes']\n",
    "\n",
    "# IMPROVEMENT: Enhanced precipitation features\n",
    "# 1. Calculate rolling statistics with more window sizes\n",
    "precip_related_cols = [col for col in df.columns if any(x in col.lower()\n",
    "                      for x in ['precipitation', 'rain', 'humid', 'pressure', 'wind'])]\n",
    "\n",
    "print(f\"\\nPrecipitation-related columns found: {precip_related_cols}\")\n",
    "\n",
    "# More fine-grained window sizes for better pattern detection\n",
    "window_sizes = [3, 5, 7, 10, 14, 21, 30]\n",
    "\n",
    "for col in precip_related_cols:\n",
    "    if col in df.columns and df[col].dtype.kind in 'ifc':  # Numeric columns only\n",
    "        # Calculate rolling statistics\n",
    "        for window in window_sizes:\n",
    "            if len(df) > window:\n",
    "                df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
    "                df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
    "\n",
    "                # Add median and max for better extreme event capture\n",
    "                df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
    "                df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
    "\n",
    "        # Calculate day-to-day changes (first derivative)\n",
    "        df[f'{col}_change'] = df[col].diff().fillna(0)\n",
    "\n",
    "        # Calculate change acceleration (second derivative)\n",
    "        df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
    "\n",
    "        # Exponential weighted features - give more weight to recent values\n",
    "        df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
    "        df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
    "\n",
    "# 2. Add more sophisticated precipitation event features\n",
    "if 'precipitation_sum' in df.columns:\n",
    "    # Define different precipitation thresholds\n",
    "    df['light_rain'] = ((df['precipitation_sum'] > 0.1) & (df['precipitation_sum'] <= 2.5)).astype(int)\n",
    "    df['moderate_rain'] = ((df['precipitation_sum'] > 2.5) & (df['precipitation_sum'] <= 10)).astype(int)\n",
    "    df['heavy_rain'] = ((df['precipitation_sum'] > 10) & (df['precipitation_sum'] <= 50)).astype(int)\n",
    "    df['extreme_rain'] = (df['precipitation_sum'] > 50).astype(int)\n",
    "\n",
    "    # Calculate rain streak features\n",
    "    rain_streak = 0\n",
    "    rain_streaks = []\n",
    "\n",
    "    for rain in df['precipitation_sum'] > 0.5:\n",
    "        if rain:\n",
    "            rain_streak += 1\n",
    "        else:\n",
    "            rain_streak = 0\n",
    "        rain_streaks.append(rain_streak)\n",
    "\n",
    "    df['rain_streak'] = rain_streaks\n",
    "\n",
    "    # Calculate frequency features for different intensities\n",
    "    for window in window_sizes:\n",
    "        if len(df) > window:\n",
    "            df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "            # Also calculate total rain days\n",
    "            df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
    "\n",
    "# 3. Weather pattern changes with more context\n",
    "if 'weathercode' in df.columns:\n",
    "    df['weather_change'] = (df['weathercode'].diff() != 0).astype(int)\n",
    "\n",
    "    # Get dominant weather pattern in last week\n",
    "    if len(df) > 7:\n",
    "        df['dominant_weathercode_7d'] = df['weathercode'].rolling(window=7).apply(\n",
    "            lambda x: x.value_counts().index[0] if not x.empty else np.nan)\n",
    "\n",
    "        # Is current weather different from dominant pattern\n",
    "        df['weather_anomaly'] = (df['weathercode'] != df['dominant_weathercode_7d']).astype(int)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "cat_cols = ['city', 'country']\n",
    "encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded = encoder.fit_transform(df[[col]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=[f'{col}_{i}' for i in range(encoded.shape[1])])\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "        encoders[col] = encoder\n",
    "        print(f\"Encoded {col} into {encoded.shape[1]} categories\")\n",
    "\n",
    "# Feature selection - drop redundant and non-useful columns\n",
    "base_drop_cols = ['time', 'sunrise', 'sunset'] + cat_cols\n",
    "\n",
    "# Remove temperature_mean if we have max and min (derived feature)\n",
    "if 'temperature_2m_mean' in df.columns and 'temperature_2m_max' in df.columns and 'temperature_2m_min' in df.columns:\n",
    "    base_drop_cols.append('temperature_2m_mean')\n",
    "    print(\"Removing temperature_2m_mean as it's derived from max and min\")\n",
    "\n",
    "# Remove apparent_temperature_mean if we have max and min (derived feature)\n",
    "if 'apparent_temperature_mean' in df.columns and 'apparent_temperature_max' in df.columns and 'apparent_temperature_min' in df.columns:\n",
    "    base_drop_cols.append('apparent_temperature_mean')\n",
    "    print(\"Removing apparent_temperature_mean as it's derived from max and min\")\n",
    "\n",
    "# Remove snow-related features for tropical climate (Sri Lanka)\n",
    "snow_cols = [col for col in df.columns if 'snow' in col.lower()]\n",
    "base_drop_cols.extend(snow_cols)\n",
    "print(f\"Removing {len(snow_cols)} snow-related features as they're irrelevant for Sri Lanka\")\n",
    "\n",
    "# IMPROVEMENT: Try to fetch ENSO data (simplified version)\n",
    "try:\n",
    "    # This is a simplified example - you would need to use a proper API or dataset\n",
    "    # Normally you'd get this from NOAA or similar source\n",
    "    # For this example, we'll create simulated ENSO data\n",
    "\n",
    "    # Generate synthetic ENSO index for the date range in the dataset\n",
    "    start_date = df['time'].min()\n",
    "    end_date = df['time'].max()\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "\n",
    "    # Create a synthetic oscillating ENSO pattern\n",
    "    enso_values = 0.5 * np.sin(np.linspace(0, 4*np.pi, len(date_range)))\n",
    "    enso_df = pd.DataFrame({\n",
    "        'date': date_range,\n",
    "        'enso_index': enso_values\n",
    "    })\n",
    "\n",
    "    # Resample to daily frequency with forward fill\n",
    "    enso_df = enso_df.set_index('date')\n",
    "    daily_enso = enso_df.resample('D').ffill()\n",
    "\n",
    "    # Merge with main dataframe\n",
    "    df_with_date = df.copy()\n",
    "    df_with_date['date'] = df_with_date['time'].dt.date\n",
    "    df_with_date['date'] = pd.to_datetime(df_with_date['date'])\n",
    "\n",
    "    # Merge ENSO data\n",
    "    df_with_enso = pd.merge(df_with_date, daily_enso, left_on='date', right_index=True, how='left')\n",
    "\n",
    "    # Replace the main dataframe if merge was successful\n",
    "    if 'enso_index' in df_with_enso.columns:\n",
    "        df = df_with_enso\n",
    "        print(\"Added ENSO index data\")\n",
    "\n",
    "    # Clean up temporary date column\n",
    "    if 'date' in df.columns:\n",
    "        df = df.drop('date', axis=1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch ENSO data: {e}\")\n",
    "    print(\"Proceeding without ENSO indices\")\n",
    "\n",
    "# Define forecast target(s)\n",
    "target_cols = ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum']\n",
    "\n",
    "# If we transformed precipitation earlier, use the original for evaluation\n",
    "if 'precipitation_sum_original' in df.columns:\n",
    "    target_cols = ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum_original']\n",
    "\n",
    "binary_precip_col = ['light_rain', 'moderate_rain', 'heavy_rain', 'extreme_rain']\n",
    "print(f\"\\nTarget columns: {target_cols}\")\n",
    "print(f\"Binary precipitation targets: {binary_precip_col}\")\n",
    "\n",
    "# Select features\n",
    "feature_cols = [col for col in df.columns if col not in base_drop_cols\n",
    "               and col not in target_cols\n",
    "               and col not in binary_precip_col\n",
    "               and col != 'precipitation_sum_original']  # Exclude original precip if we created it\n",
    "\n",
    "print(f\"\\nSelected feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols[:10], \"... and more\")\n",
    "\n",
    "# Print summary stats for precipitation\n",
    "if 'precipitation_sum_original' in df.columns:\n",
    "    print(\"\\nPrecipitation Summary Statistics:\")\n",
    "    print(df['precipitation_sum_original'].describe())\n",
    "\n",
    "    print(f\"Days with light rain: {df['light_rain'].sum()} ({df['light_rain'].mean()*100:.1f}%)\")\n",
    "    print(f\"Days with moderate rain: {df['moderate_rain'].sum()} ({df['moderate_rain'].mean()*100:.1f}%)\")\n",
    "    print(f\"Days with heavy rain: {df['heavy_rain'].sum()} ({df['heavy_rain'].mean()*100:.1f}%)\")\n",
    "    print(f\"Days with extreme rain: {df['extreme_rain'].sum()} ({df['extreme_rain'].mean()*100:.1f}%)\")\n",
    "\n",
    "# IMPROVEMENT: Set cutoff dates based on monsoon seasons for better model testing\n",
    "# We want the test set to include both SW and NE monsoon periods\n",
    "df = df.sort_values('time')\n",
    "\n",
    "# Use 80-10-10 split but ensure test set has representative monsoon seasons\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size+val_size]\n",
    "test_df = df.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_df)} rows (from {train_df['time'].min()} to {train_df['time'].max()})\")\n",
    "print(f\"Validation set: {len(val_df)} rows (from {val_df['time'].min()} to {val_df['time'].max()})\")\n",
    "print(f\"Test set: {len(test_df)} rows (from {test_df['time'].min()} to {test_df['time'].max()})\")\n",
    "\n",
    "# Check monsoon representation in test set\n",
    "if 'southwest_monsoon' in test_df.columns and 'northeast_monsoon' in test_df.columns:\n",
    "    sw_monsoon_days = test_df['southwest_monsoon'].sum()\n",
    "    ne_monsoon_days = test_df['northeast_monsoon'].sum()\n",
    "    print(f\"Test set includes {sw_monsoon_days} SW monsoon days and {ne_monsoon_days} NE monsoon days\")\n",
    "\n",
    "# IMPROVEMENT: Use more sophisticated scaling techniques\n",
    "# For precipitation, we already applied sinh-arcsinh transformation\n",
    "# For other features, we'll use a robust scaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Use robust scaling for better handling of outliers\n",
    "scaler_features = RobustScaler()  # Less sensitive to outliers than StandardScaler\n",
    "scaler_targets = RobustScaler()   # Better preserves the distribution shape\n",
    "\n",
    "# Fit scalers on training data only\n",
    "train_features = train_df[feature_cols].copy()\n",
    "train_targets = train_df[target_cols].copy()\n",
    "train_binary_target = train_df[binary_precip_col].copy()\n",
    "\n",
    "# Transform all datasets - features\n",
    "train_features_scaled = scaler_features.fit_transform(train_features)\n",
    "val_features_scaled = scaler_features.transform(val_df[feature_cols])\n",
    "test_features_scaled = scaler_features.transform(test_df[feature_cols])\n",
    "\n",
    "# Transform all datasets - targets\n",
    "train_targets_scaled = scaler_targets.fit_transform(train_targets)\n",
    "val_targets_scaled = scaler_targets.transform(val_df[target_cols])\n",
    "test_targets_scaled = scaler_targets.transform(test_df[target_cols])\n",
    "\n",
    "# Binary target doesn't need scaling, but we need to concatenate multiple binary targets\n",
    "train_binary = np.hstack([train_df[col].values.reshape(-1, 1) for col in binary_precip_col])\n",
    "val_binary = np.hstack([val_df[col].values.reshape(-1, 1) for col in binary_precip_col])\n",
    "test_binary = np.hstack([test_df[col].values.reshape(-1, 1) for col in binary_precip_col])\n",
    "\n",
    "# IMPROVEMENT: Longer sequence length (60 days) to better capture seasonal patterns\n",
    "# Create sequences for LSTM with increased sequence length\n",
    "def create_sequences(features, targets, binary_targets=None, seq_length=60, pred_length=5):\n",
    "    \"\"\"Create sequences with optional binary target output\"\"\"\n",
    "    X, y = [], []\n",
    "    binary_y = []\n",
    "\n",
    "    for i in range(len(features) - seq_length - pred_length + 1):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(targets[i+seq_length:i+seq_length+pred_length])\n",
    "\n",
    "        if binary_targets is not None:\n",
    "            binary_y.append(binary_targets[i+seq_length:i+seq_length+pred_length])\n",
    "\n",
    "    if binary_targets is not None:\n",
    "        return np.array(X), np.array(y), np.array(binary_y)\n",
    "    else:\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters - INCREASED SEQUENCE LENGTH for better pattern capture\n",
    "sequence_length = 60  # Increased from 30 to 60 days\n",
    "prediction_length = 5  # Predict 5 days ahead\n",
    "\n",
    "# Create sequences with binary targets\n",
    "X_train, y_train, binary_y_train = create_sequences(\n",
    "    train_features_scaled, train_targets_scaled, train_binary,\n",
    "    seq_length=sequence_length, pred_length=prediction_length)\n",
    "\n",
    "X_val, y_val, binary_y_val = create_sequences(\n",
    "    val_features_scaled, val_targets_scaled, val_binary,\n",
    "    seq_length=sequence_length, pred_length=prediction_length)\n",
    "\n",
    "X_test, y_test, binary_y_test = create_sequences(\n",
    "    test_features_scaled, test_targets_scaled, test_binary,\n",
    "    seq_length=sequence_length, pred_length=prediction_length)\n",
    "\n",
    "print(f\"\\nSequence input shape: {X_train.shape}\")\n",
    "print(f\"Sequence output shape: {y_train.shape}\")\n",
    "print(f\"Binary output shape: {binary_y_train.shape}\")\n",
    "\n",
    "# Flatten target arrays for model training\n",
    "y_train_flat = y_train.reshape(y_train.shape[0], -1)\n",
    "y_val_flat = y_val.reshape(y_val.shape[0], -1)\n",
    "y_test_flat = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "# IMPROVEMENT: Asymmetric loss function that penalizes precipitation underestimation more heavily\n",
    "def asymmetric_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function that penalizes underestimation of precipitation (false negatives)\n",
    "    more heavily than overestimation (false positives).\n",
    "\n",
    "    For precipitation values (every 3rd value starting at index 2), we apply higher\n",
    "    weight when the prediction is lower than the actual value.\n",
    "    \"\"\"\n",
    "    # Calculate squared errors\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "\n",
    "    # Extract precipitation values (every 3rd column starting from index 2)\n",
    "    precip_indices = tf.range(2, tf.shape(y_true)[1], 3)\n",
    "\n",
    "    # Create weights tensor (default weight = 1.0)\n",
    "    weights = tf.ones_like(squared_error)\n",
    "\n",
    "    # For each precipitation column\n",
    "    for i in range(0, prediction_length):\n",
    "        idx = 2 + i * 3  # Index of precipitation column\n",
    "\n",
    "        if idx < tf.shape(y_true)[1]:\n",
    "            # Extract precipitation values\n",
    "            true_vals = y_true[:, idx]\n",
    "            pred_vals = y_pred[:, idx]\n",
    "\n",
    "            # Calculate error direction: positive error means underestimation\n",
    "            error = true_vals - pred_vals\n",
    "\n",
    "            # Create weights: 2.5x for underestimation, 1.0 for overestimation\n",
    "            precip_weights = tf.where(error > 0, 2.5, 1.0)\n",
    "\n",
    "            # Create indices for updating the weights tensor\n",
    "            indices = tf.stack([tf.range(tf.shape(weights)[0]), tf.ones_like(tf.range(tf.shape(weights)[0])) * idx], axis=1)\n",
    "\n",
    "            # Update weights for this precipitation column\n",
    "            weights = tf.tensor_scatter_nd_update(weights, indices, precip_weights)\n",
    "\n",
    "    # Apply weights to squared errors and take mean\n",
    "    weighted_squared_error = squared_error * weights\n",
    "    return tf.reduce_mean(weighted_squared_error)\n",
    "\n",
    "# IMPROVEMENT: Residual ConvLSTM model with attention for better precipitation forecasting\n",
    "def create_advanced_model(input_shape, num_targets, num_days):\n",
    "    \"\"\"\n",
    "    Create an advanced model with:\n",
    "    1. Convolutional layers to extract spatial patterns\n",
    "    2. LSTM layers with residual connections\n",
    "    3. Self-attention mechanism\n",
    "    4. Multi-headed regression\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # 1D CNN layers to extract features\n",
    "    x = Conv1D(64, kernel_size=5, padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Second CNN layer\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Store the CNN output for residual connection\n",
    "    cnn_output = x\n",
    "\n",
    "    # Bidirectional LSTM with increased capacity\n",
    "    x = Bidirectional(LSTM(192, return_sequences=True, activation='tanh'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Self-attention mechanism\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=4, key_dim=48\n",
    "    )(x, x)\n",
    "\n",
    "    # Add residual connection around attention\n",
    "    x = Add()([attention_output, x])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second LSTM layer\n",
    "    x = Bidirectional(LSTM(128, return_sequences=False, activation='tanh'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Residual connection from CNN (need to match dimensions)\n",
    "    # Global average pooling to reduce CNN output dimensions\n",
    "    cnn_pooled = tf.keras.layers.GlobalAveragePooling1D()(cnn_output)\n",
    "    cnn_pooled = Dense(256)(cnn_pooled)  # Match dimensions with LSTM output\n",
    "\n",
    "    # Combine LSTM output with CNN features\n",
    "    x = Add()([x, cnn_pooled])\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Common dense layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Classification outputs for rain intensity categories\n",
    "    rain_outputs = []\n",
    "    for i in range(num_days):\n",
    "        for j, category in enumerate(['light', 'moderate', 'heavy', 'extreme']):\n",
    "            rain_output = Dense(16, activation='relu')(x)\n",
    "            rain_output = Dense(1, activation='sigmoid',\n",
    "                               name=f'day{i+1}_{category}_rain')(rain_output)\n",
    "            rain_outputs.append(rain_output)\n",
    "\n",
    "    # Main regression output\n",
    "    regression_output = Dense(num_targets * num_days, name='main_output')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=rain_outputs + [regression_output])\n",
    "\n",
    "    # Prepare loss dictionary\n",
    "    losses = {'main_output': asymmetric_mse}\n",
    "    loss_weights = {'main_output': 1.0}\n",
    "\n",
    "    # Add binary crossentropy for all rain category outputs\n",
    "    for i in range(len(rain_outputs)):\n",
    "        output_name = model.outputs[i].name.split('/')[0]\n",
    "        losses[output_name] = 'binary_crossentropy'\n",
    "        loss_weights[output_name] = 0.1  # Lower weight for classification tasks\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=losses,\n",
    "        loss_weights=loss_weights,\n",
    "        metrics={'main_output': 'mae'}\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# IMPROVEMENT: Quantile regression model for better extreme event capture\n",
    "def create_quantile_regression_model(input_shape, num_targets, num_days):\n",
    "    \"\"\"Create a model that predicts multiple quantiles (50th, 90th) for precipitation\"\"\"\n",
    "\n",
    "    # Define quantile loss function\n",
    "    def quantile_loss(q, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        return K.mean(K.maximum(q * error, (q - 1) * error))\n",
    "\n",
    "    # Median (q=0.5) quantile loss\n",
    "    q50_loss = lambda y, f: quantile_loss(0.5, y, f)\n",
    "    # Upper (q=0.9) quantile loss for extreme events\n",
    "    q90_loss = lambda y, f: quantile_loss(0.9, y, f)\n",
    "\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Shared layers\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Common dense layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Median (q=0.5) output - standard predictions\n",
    "    median_output = Dense(num_targets * num_days, name='q50_output')(x)\n",
    "\n",
    "    # Upper quantile (q=0.9) output - helps with extreme precipitation\n",
    "    upper_output = Dense(num_targets * num_days, name='q90_output')(x)\n",
    "\n",
    "    # Create model with multiple outputs\n",
    "    model = Model(inputs=inputs, outputs=[median_output, upper_output])\n",
    "\n",
    "    # Compile with appropriate losses\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={'q50_output': q50_loss, 'q90_output': q90_loss},\n",
    "        loss_weights={'q50_output': 1.0, 'q90_output': 0.5},\n",
    "        metrics={'q50_output': 'mae', 'q90_output': 'mae'}\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Standard Enhanced model (updated with higher capacity)\n",
    "def create_enhanced_model(input_shape, output_shape):\n",
    "    \"\"\"Create an enhanced LSTM model with higher capacity\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # First layer: larger capacity (256 units)\n",
    "    model.add(LSTM(256, activation='tanh', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Second layer\n",
    "    model.add(LSTM(192, activation='tanh', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Third layer\n",
    "    model.add(LSTM(128, activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(output_shape))\n",
    "\n",
    "    # Compile with asymmetric loss function\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=asymmetric_mse,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the advanced model\n",
    "def train_advanced_model(X_train, y_train, binary_y_train,\n",
    "                        X_val, y_val, binary_y_val,\n",
    "                        X_test, y_test, binary_y_test):\n",
    "    \"\"\"Train the advanced model\"\"\"\n",
    "\n",
    "    print(\"\\n\\nTraining ADVANCED model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    num_targets = y_train.shape[2]  # temp_max, temp_min, precip\n",
    "    num_days = y_train.shape[1]     # 5 days\n",
    "\n",
    "    # Create model\n",
    "    model = create_advanced_model(input_shape, num_targets, num_days)\n",
    "\n",
    "    # Prepare classification targets - reshape to match output structure\n",
    "    binary_targets_train = []\n",
    "    binary_targets_val = []\n",
    "\n",
    "    # For each prediction day and rain category\n",
    "    for i in range(num_days):\n",
    "        for j in range(binary_y_train.shape[2]):  # 4 categories: light, moderate, heavy, extreme\n",
    "            binary_targets_train.append(binary_y_train[:, i, j])\n",
    "            binary_targets_val.append(binary_y_val[:, i, j])\n",
    "\n",
    "    # Add main regression target\n",
    "    train_targets = binary_targets_train + [y_train_flat]\n",
    "    val_targets = binary_targets_val + [y_val_flat]\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_advanced_weather_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train model with more epochs and reduced batch size\n",
    "    history = model.fit(\n",
    "        X_train, train_targets,\n",
    "        validation_data=(X_val, val_targets),\n",
    "        epochs=100,  # More epochs with early stopping\n",
    "        batch_size=16,  # Smaller batch size for better generalization\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Prepare test targets for evaluation\n",
    "    binary_targets_test = []\n",
    "    for i in range(num_days):\n",
    "        for j in range(binary_y_test.shape[2]):\n",
    "            binary_targets_test.append(binary_y_test[:, i, j])\n",
    "\n",
    "    test_targets = binary_targets_test + [y_test_flat]\n",
    "    test_results = model.evaluate(X_test, test_targets, verbose=0)\n",
    "\n",
    "    # Main output is the last one\n",
    "    main_output_idx = len(model.output_names) - 1\n",
    "    test_loss = test_results[0]  # Overall loss\n",
    "    main_mae = test_results[main_output_idx + 1]  # MAE for main output\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    main_predictions = predictions[-1]  # Last output is main regression\n",
    "\n",
    "    # Reshape to original dimensions\n",
    "    main_predictions_reshaped = main_predictions.reshape(y_test.shape)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Format classification predictions\n",
    "    binary_predictions = []\n",
    "    binary_accuracies = []\n",
    "\n",
    "    # Process each binary output (one per day per rain category)\n",
    "    curr_pred_idx = 0\n",
    "    for i in range(num_days):\n",
    "        day_preds = []\n",
    "        day_accs = []\n",
    "\n",
    "        for j in range(binary_y_test.shape[2]):\n",
    "            pred = (predictions[curr_pred_idx] > 0.5).astype(int)\n",
    "            true = binary_y_test[:, i, j]\n",
    "            acc = accuracy_score(true, pred)\n",
    "\n",
    "            day_preds.append(pred)\n",
    "            day_accs.append(acc)\n",
    "            curr_pred_idx += 1\n",
    "\n",
    "        binary_predictions.append(day_preds)\n",
    "        binary_accuracies.append(day_accs)\n",
    "\n",
    "    # Calculate average accuracy by rain category\n",
    "    rain_categories = ['light', 'moderate', 'heavy', 'extreme']\n",
    "    category_accuracies = {}\n",
    "\n",
    "    for j, category in enumerate(rain_categories):\n",
    "        # Average across all days for this category\n",
    "        category_acc = np.mean([binary_accuracies[i][j] for i in range(num_days)])\n",
    "        category_accuracies[category] = category_acc\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'test_mae': main_mae,\n",
    "        'predictions': main_predictions_reshaped,\n",
    "        'binary_predictions': binary_predictions,\n",
    "        'category_accuracies': category_accuracies,\n",
    "        'avg_accuracy': np.mean(list(category_accuracies.values())),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\nADVANCED Model Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Regression Test MAE: {main_mae:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    print(\"\\nAccuracy by Rain Category:\")\n",
    "    for category, acc in category_accuracies.items():\n",
    "        print(f\"  {category.capitalize()}: {acc:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Train the quantile regression model\n",
    "def train_quantile_model(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train the quantile regression model\"\"\"\n",
    "\n",
    "    print(\"\\n\\nTraining QUANTILE model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    num_targets = y_train.shape[2]\n",
    "    num_days = y_train.shape[1]\n",
    "\n",
    "    # Create model\n",
    "    model = create_quantile_regression_model(input_shape, num_targets, num_days)\n",
    "\n",
    "    # Train model\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_quantile_weather_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, [y_train_flat, y_train_flat],  # Same targets for both outputs during training\n",
    "        validation_data=(X_val, [y_val_flat, y_val_flat]),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    test_results = model.evaluate(X_test, [y_test_flat, y_test_flat], verbose=0)\n",
    "    test_loss = test_results[0]  # Overall loss\n",
    "    q50_mae = test_results[2]    # MAE for median predictions\n",
    "    q90_mae = test_results[4]    # MAE for upper quantile predictions\n",
    "\n",
    "    # Make predictions\n",
    "    q50_pred, q90_pred = model.predict(X_test)\n",
    "\n",
    "    # Reshape predictions to original dimensions\n",
    "    q50_pred_reshaped = q50_pred.reshape(y_test.shape)\n",
    "    q90_pred_reshaped = q90_pred.reshape(y_test.shape)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'q50_mae': q50_mae,\n",
    "        'q90_mae': q90_mae,\n",
    "        'q50_predictions': q50_pred_reshaped,\n",
    "        'q90_predictions': q90_pred_reshaped,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\nQUANTILE Model Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Median (q50) Test MAE: {q50_mae:.4f}\")\n",
    "    print(f\"Upper (q90) Test MAE: {q90_mae:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Train the enhanced baseline model\n",
    "def train_enhanced_baseline(X_train, y_train_flat, X_val, y_val_flat, X_test, y_test):\n",
    "    \"\"\"Train an enhanced baseline model with higher capacity\"\"\"\n",
    "\n",
    "    print(\"\\n\\nTraining ENHANCED BASELINE model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    output_shape = y_train_flat.shape[1]\n",
    "\n",
    "    # Create model\n",
    "    model = create_enhanced_model(input_shape, output_shape)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_enhanced_baseline_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train_flat,\n",
    "        validation_data=(X_val, y_val_flat),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_mae = model.evaluate(X_test, y_test_flat)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_flat = model.predict(X_test)\n",
    "    predictions = predictions_flat.reshape(y_test.shape)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'test_mae': test_mae,\n",
    "        'predictions': predictions,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\nENHANCED BASELINE Model Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Train all models\n",
    "print(\"\\n\\n==== TRAINING SRI LANKA PRECIPITATION MODELS ====\")\n",
    "enhanced_result = train_enhanced_baseline(\n",
    "    X_train, y_train_flat,\n",
    "    X_val, y_val_flat,\n",
    "    X_test, y_test_flat\n",
    ")\n",
    "\n",
    "advanced_result = train_advanced_model(\n",
    "    X_train, y_train, binary_y_train,\n",
    "    X_val, y_val, binary_y_val,\n",
    "    X_test, y_test, binary_y_test\n",
    ")\n",
    "\n",
    "quantile_result = train_quantile_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "# IMPROVEMENT: Create ensemble model that combines predictions from all models\n",
    "# with special focus on high precipitation events\n",
    "def create_advanced_ensemble_predictions(models_dict, y_test):\n",
    "    \"\"\"\n",
    "    Create ensemble predictions by combining all models with intelligent weighting\n",
    "    based on precipitation intensity:\n",
    "    - For low precipitation: Give more weight to median predictions\n",
    "    - For high precipitation: Give more weight to upper quantile and advanced model\n",
    "    \"\"\"\n",
    "    # Extract predictions\n",
    "    enhanced_preds = models_dict['enhanced']['predictions']\n",
    "    advanced_preds = models_dict['advanced']['predictions']\n",
    "    q50_preds = models_dict['quantile']['q50_predictions']\n",
    "    q90_preds = models_dict['quantile']['q90_predictions']\n",
    "\n",
    "    # Create ensemble predictions array\n",
    "    ensemble_predictions = np.zeros_like(y_test)\n",
    "\n",
    "    # For each sample, day, and target\n",
    "    for sample_idx in range(y_test.shape[0]):\n",
    "        for day_idx in range(y_test.shape[1]):\n",
    "            for target_idx in range(y_test.shape[2]):\n",
    "                # Is this a precipitation prediction?\n",
    "                is_precip = (target_idx == 2)\n",
    "\n",
    "                if is_precip:\n",
    "                    # Compute ensemble prediction for precipitation with adaptive weighting\n",
    "\n",
    "                    # Get all precipitation predictions for this point\n",
    "                    enhanced_val = enhanced_preds[sample_idx, day_idx, target_idx]\n",
    "                    advanced_val = advanced_preds[sample_idx, day_idx, target_idx]\n",
    "                    q50_val = q50_preds[sample_idx, day_idx, target_idx]\n",
    "                    q90_val = q90_preds[sample_idx, day_idx, target_idx]\n",
    "\n",
    "                    # Calculate maximum predicted value\n",
    "                    max_pred = max(enhanced_val, advanced_val, q50_val)\n",
    "\n",
    "                    # Use different weighting strategies based on predicted precipitation\n",
    "                    if max_pred > 0.75:  # High precipitation likely (in normalized scale)\n",
    "                        # Give more weight to q90 and advanced model\n",
    "                        weights = {\n",
    "                            'enhanced': 0.1,\n",
    "                            'advanced': 0.4,\n",
    "                            'q50': 0.1,\n",
    "                            'q90': 0.4\n",
    "                        }\n",
    "                    else:  # Low/normal precipitation\n",
    "                        # More balanced weighting\n",
    "                        weights = {\n",
    "                            'enhanced': 0.25,\n",
    "                            'advanced': 0.25,\n",
    "                            'q50': 0.35,\n",
    "                            'q90': 0.15\n",
    "                        }\n",
    "\n",
    "                    # Calculate weighted ensemble\n",
    "                    ensemble_val = (\n",
    "                        weights['enhanced'] * enhanced_val +\n",
    "                        weights['advanced'] * advanced_val +\n",
    "                        weights['q50'] * q50_val +\n",
    "                        weights['q90'] * q90_val\n",
    "                    )\n",
    "\n",
    "                    ensemble_predictions[sample_idx, day_idx, target_idx] = ensemble_val\n",
    "                else:\n",
    "                    # For temperature: Simple average of all models except q90\n",
    "                    ensemble_predictions[sample_idx, day_idx, target_idx] = (\n",
    "                        enhanced_preds[sample_idx, day_idx, target_idx] * 0.35 +\n",
    "                        advanced_preds[sample_idx, day_idx, target_idx] * 0.35 +\n",
    "                        q50_preds[sample_idx, day_idx, target_idx] * 0.3\n",
    "                    )\n",
    "\n",
    "    return ensemble_predictions\n",
    "\n",
    "# Collect all model results\n",
    "models_dict = {\n",
    "    'enhanced': enhanced_result,\n",
    "    'advanced': advanced_result,\n",
    "    'quantile': quantile_result\n",
    "}\n",
    "\n",
    "# Create advanced ensemble predictions\n",
    "ensemble_predictions = create_advanced_ensemble_predictions(models_dict, y_test)\n",
    "\n",
    "# Calculate metrics for all models including ensemble\n",
    "def inverse_transform_predictions(predictions, targets, scaler):\n",
    "    \"\"\"Transform scaled values back to original range\"\"\"\n",
    "    pred_reshaped = predictions.reshape(-1, targets.shape[-1])\n",
    "    targets_reshaped = targets.reshape(-1, targets.shape[-1])\n",
    "\n",
    "    pred_original = scaler.inverse_transform(pred_reshaped)\n",
    "    targets_original = targets_original.reshape(targets.shape)\n",
    "\n",
    "    return pred_original, targets_original\n",
    "\n",
    "# Process all models to get metrics\n",
    "detailed_metrics = {}\n",
    "\n",
    "# Process enhanced baseline\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    enhanced_result['predictions'], y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['enhanced'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Process advanced model\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    advanced_result['predictions'], y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['advanced'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Process quantile model (q50 - median predictions)\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    quantile_result['q50_predictions'], y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['quantile'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Process ensemble model\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    ensemble_predictions, y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['ensemble'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Calculate overall model metrics\n",
    "print(\"\\n\\n==== PRECIPITATION FORECAST EVALUATION ====\")\n",
    "print(\"\\nPrecipitation Prediction Performance:\")\n",
    "for model_type, result in detailed_metrics.items():\n",
    "    precip_metrics = result['metrics'][target_cols[2]]  # Use correct target name\n",
    "    avg_mae = np.mean([d['MAE'] for d in precip_metrics.values()])\n",
    "    avg_rmse = np.mean([d['RMSE'] for d in precip_metrics.values()])\n",
    "    print(f\"{model_type.upper()}: Avg MAE = {avg_mae:.2f}, Avg RMSE = {avg_rmse:.2f}\")\n",
    "\n",
    "# Find best model for each target\n",
    "print(\"\\nBest model for each target variable:\")\n",
    "for var_idx, var_name in enumerate(target_cols):\n",
    "    var_maes = {}\n",
    "    for model_type, result in detailed_metrics.items():\n",
    "        avg_mae = np.mean([result['metrics'][var_name][f'Day {day}']['MAE'] for day in range(1, prediction_length + 1)])\n",
    "        var_maes[model_type] = avg_mae\n",
    "\n",
    "    best_model = min(var_maes, key=var_maes.get)\n",
    "    print(f\"{var_name}: {best_model.upper()} (MAE: {var_maes[best_model]:.2f})\")\n",
    "\n",
    "# Plot comparison of precipitation predictions\n",
    "def plot_precipitation_comparison(detailed_metrics, variable_name):\n",
    "    \"\"\"Compare precipitation predictions across models\"\"\"\n",
    "    var_idx = target_cols.index(variable_name)\n",
    "    sample_indices = [0, 1, 2, 3, 4]  # First 5 samples\n",
    "\n",
    "    model_colors = {\n",
    "        'enhanced': 'c--',\n",
    "        'advanced': 'm--',\n",
    "        'quantile': 'g--',\n",
    "        'ensemble': 'y--'\n",
    "    }\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # Get actual values (same for all models)\n",
    "        true_values = list(detailed_metrics.values())[0]['targets_original'][idx, :, var_idx]\n",
    "        plt.plot(range(len(true_values)), true_values, 'bo-', linewidth=2, label='Actual')\n",
    "\n",
    "        # Plot predicted values for each model\n",
    "        for model_type, result in detailed_metrics.items():\n",
    "            pred_values = result['predictions_original'][idx, :, var_idx]\n",
    "            plt.plot(range(len(pred_values)), pred_values, model_colors[model_type],\n",
    "                     linewidth=2, label=f'{model_type.capitalize()}')\n",
    "\n",
    "        plt.title(f'Sri Lanka {variable_name.title()} - Sample {idx+1}')\n",
    "        plt.ylabel(f'{variable_name}')\n",
    "        plt.xlabel('Day')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'sri_lanka_{variable_name}_sample_{idx+1}.png')\n",
    "        plt.show()\n",
    "\n",
    "# Find high precipitation samples\n",
    "def find_high_precip_samples(detailed_metrics, threshold=15):\n",
    "    \"\"\"Find samples with high precipitation values\"\"\"\n",
    "    var_idx = target_cols.index(target_cols[2])  # Use correct target name\n",
    "    targets = list(detailed_metrics.values())[0]['targets_original']\n",
    "\n",
    "    high_precip_indices = []\n",
    "    for i in range(len(targets)):\n",
    "        if np.max(targets[i, :, var_idx]) > threshold:\n",
    "            high_precip_indices.append(i)\n",
    "\n",
    "    return high_precip_indices[:5]  # Return at most 5 samples\n",
    "\n",
    "# Plot precipitation predictions\n",
    "print(\"\\nPlotting precipitation comparison...\")\n",
    "plot_precipitation_comparison(detailed_metrics, target_cols[2])  # Use correct target name\n",
    "\n",
    "# Plot high precipitation events\n",
    "high_precip_indices = find_high_precip_samples(detailed_metrics)\n",
    "print(f\"\\nFound {len(high_precip_indices)} samples with high precipitation\")\n",
    "\n",
    "if high_precip_indices:\n",
    "    for idx in high_precip_indices:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        var_idx = target_cols.index(target_cols[2])  # Use correct target name\n",
    "\n",
    "        # Plot actual values\n",
    "        true_values = list(detailed_metrics.values())[0]['targets_original'][idx, :, var_idx]\n",
    "        plt.plot(range(len(true_values)), true_values, 'bo-', linewidth=2, label='Actual')\n",
    "\n",
    "        # Plot predicted values for each model\n",
    "        model_colors = {\n",
    "            'enhanced': 'c--',\n",
    "            'advanced': 'm--',\n",
    "            'quantile': 'g--',\n",
    "            'ensemble': 'y--'\n",
    "        }\n",
    "\n",
    "        for model_type, result in detailed_metrics.items():\n",
    "            pred_values = result['predictions_original'][idx, :, var_idx]\n",
    "            plt.plot(range(len(pred_values)), pred_values, model_colors[model_type],\n",
    "                     linewidth=2, label=f'{model_type.capitalize()}')\n",
    "\n",
    "        plt.title(f'Sri Lanka High Precipitation Event - Sample {idx}')\n",
    "        plt.ylabel('Precipitation (mm)')\n",
    "        plt.xlabel('Day')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'sri_lanka_high_precipitation_sample_{idx}.png')\n",
    "        plt.show()\n",
    "\n",
    "# Print precipitation performance by day\n",
    "print(\"\\nPrecipitation Prediction Performance by Day:\")\n",
    "for day in range(1, prediction_length + 1):\n",
    "    print(f\"\\nDay {day}:\")\n",
    "    for model_type, result in detailed_metrics.items():\n",
    "        mae = result['metrics'][target_cols[2]][f'Day {day}']['MAE']  # Use correct target name\n",
    "        rmse = result['metrics'][target_cols[2]][f'Day {day}']['RMSE']\n",
    "        print(f\"  {model_type.upper()}: MAE = {mae:.2f}, RMSE = {rmse:.2f}\")\n",
    "\n",
    "# Generate final model comparison summary\n",
    "print(\"\\n\\n===== MODEL COMPARISON SUMMARY =====\")\n",
    "print(\"Overall Metrics:\")\n",
    "\n",
    "# Print details for each model\n",
    "print(f\"ENHANCED: MAE = {enhanced_result['test_mae']:.4f}, Training time: {enhanced_result['training_time']:.2f} seconds\")\n",
    "print(f\"ADVANCED: MAE = {advanced_result['test_mae']:.4f}, Training time: {advanced_result['training_time']:.2f} seconds\")\n",
    "print(f\"QUANTILE (q50): MAE = {quantile_result['q50_mae']:.4f}, Training time: {quantile_result['training_time']:.2f} seconds\")\n",
    "\n",
    "# Calculate ensemble MAE\n",
    "ensemble_mae = np.mean(np.abs(detailed_metrics['ensemble']['predictions_original'] -\n",
    "                            detailed_metrics['ensemble']['targets_original']))\n",
    "print(f\"ENSEMBLE: MAE = {ensemble_mae:.4f}\")\n",
    "\n",
    "# Calculate improvement over previous best model\n",
    "previous_best_mae = 3.45  # From earlier result\n",
    "best_model_type = min(detailed_metrics, key=lambda x: np.mean([d['MAE'] for d in detailed_metrics[x]['metrics'][target_cols[2]].values()]))\n",
    "best_model_mae = np.mean([d['MAE'] for d in detailed_metrics[best_model_type]['metrics'][target_cols[2]].values()])\n",
    "improvement = (previous_best_mae - best_model_mae) / previous_best_mae * 100\n",
    "\n",
    "print(f\"\\nBest model for precipitation: {best_model_type.upper()} with MAE = {best_model_mae:.2f}\")\n",
    "print(f\"Improvement over previous best model: {improvement:.1f}%\")\n",
    "\n",
    "# Save models\n",
    "enhanced_result['model'].save('sri_lanka_enhanced_precipitation_model.h5')\n",
    "print(\"Enhanced model saved as 'sri_lanka_enhanced_precipitation_model.h5'\")\n",
    "\n",
    "advanced_result['model'].save('sri_lanka_advanced_precipitation_model.h5')\n",
    "print(\"Advanced model saved as 'sri_lanka_advanced_precipitation_model.h5'\")\n",
    "\n",
    "quantile_result['model'].save('sri_lanka_quantile_precipitation_model.h5')\n",
    "print(\"Quantile model saved as 'sri_lanka_quantile_precipitation_model.h5'\")\n",
    "\n",
    "print(\"\\nAll model training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4ab90-c5c9-4d4d-8dcf-0695ffaac614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
