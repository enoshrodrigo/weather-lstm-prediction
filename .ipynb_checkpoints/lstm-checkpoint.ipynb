{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0eb5154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe8d2f-ae31-4bef-9332-9b198bbf0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Input\n",
    "from tensorflow.keras.layers import Conv1D, TimeDistributed, MaxPooling1D, Flatten, Add, Attention, Reshape\n",
    "from tensorflow.keras.layers import ConvLSTM2D, RepeatVector, Concatenate, Lambda, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, accuracy_score\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Enable memory growth for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU.\")\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    # For Google Colab\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    df = pd.read_csv(filename)\n",
    "except ImportError:\n",
    "    # For local execution\n",
    "    df = pd.read_csv('weather_data.csv')  # Update with your file path\n",
    "\n",
    "# Filter for Sri Lanka data\n",
    "if 'country' in df.columns:\n",
    "    original_len = len(df)\n",
    "    df = df[df['country'] == 'Sri Lanka']\n",
    "    print(f\"Filtered data for Sri Lanka: {len(df)} rows (from {original_len} total)\")\n",
    "    if len(df) == 0:\n",
    "        print(\"WARNING: No Sri Lanka data found. Using all available data.\")\n",
    "        df = pd.read_csv('weather_data.csv')  # Reload the data\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Convert time to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# IMPROVEMENT: Advanced data preprocessing using sinh-arcsinh transformation\n",
    "def sinh_arcsinh_transform(x, epsilon=0.01, delta=1.0):\n",
    "    \"\"\"Apply sinh-arcsinh transformation to better handle extreme values\"\"\"\n",
    "    return np.sinh(delta * np.arcsinh(x) - epsilon)\n",
    "\n",
    "def inverse_sinh_arcsinh_transform(x, epsilon=0.01, delta=1.0):\n",
    "    \"\"\"Inverse transform to return to original scale\"\"\"\n",
    "    return np.sinh((np.arcsinh(x) + epsilon) / delta)\n",
    "\n",
    "# Apply transformation to precipitation values if they exist\n",
    "if 'precipitation_sum' in df.columns:\n",
    "    # Store original values for reference\n",
    "    df['precipitation_sum_original'] = df['precipitation_sum'].copy()\n",
    "\n",
    "    # Handle zeros and small values carefully\n",
    "    mask = df['precipitation_sum'] > 0\n",
    "    df.loc[mask, 'precipitation_sum_transformed'] = sinh_arcsinh_transform(df.loc[mask, 'precipitation_sum'])\n",
    "    df.loc[~mask, 'precipitation_sum_transformed'] = 0\n",
    "\n",
    "    print(f\"Applied sinh-arcsinh transformation to precipitation values\")\n",
    "\n",
    "    # Use transformed values for modeling\n",
    "    # Note: We'll keep the original column name for simplicity in the code\n",
    "    df['precipitation_sum'] = df['precipitation_sum_transformed']\n",
    "\n",
    "# Standard time features - Moved this section up\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['dayofyear'] = df['time'].dt.dayofyear\n",
    "df['year'] = df['time'].dt.year\n",
    "df['season'] = (df['month'] % 12 + 3) // 3  # 1: spring, 2: summer, 3: fall, 4: winter\n",
    "df['week_of_year'] = df['time'].dt.isocalendar().week\n",
    "\n",
    "# IMPROVEMENT: Sri Lanka-specific feature engineering\n",
    "# 1. Add monsoon season indicators\n",
    "df['southwest_monsoon'] = ((df['month'] >= 5) & (df['month'] <= 9)).astype(int)\n",
    "df['northeast_monsoon'] = ((df['month'] >= 11) | (df['month'] <= 3)).astype(int)\n",
    "df['inter_monsoon1'] = (df['month'] == 4).astype(int)\n",
    "df['inter_monsoon2'] = (df['month'] == 10).astype(int)\n",
    "\n",
    "# 2. Add typical monsoon intensity based on historical patterns\n",
    "# This is a simplified model - ideally would be based on historical data\n",
    "monsoon_intensity = {\n",
    "    1: 0.7,  # January - NE monsoon\n",
    "    2: 0.4,  # February - NE monsoon (weakening)\n",
    "    3: 0.2,  # March - end of NE monsoon\n",
    "    4: 0.3,  # April - inter-monsoon\n",
    "    5: 0.6,  # May - start of SW monsoon\n",
    "    6: 0.8,  # June - SW monsoon\n",
    "    7: 0.9,  # July - SW monsoon peak\n",
    "    8: 0.9,  # August - SW monsoon peak\n",
    "    9: 0.7,  # September - SW monsoon (weakening)\n",
    "    10: 0.5, # October - inter-monsoon\n",
    "    11: 0.6, # November - start of NE monsoon\n",
    "    12: 0.8, # December - NE monsoon\n",
    "}\n",
    "\n",
    "df['monsoon_intensity'] = df['month'].map(monsoon_intensity)\n",
    "\n",
    "# 3. Regional indicators for Sri Lanka's climate zones\n",
    "# If city information is available\n",
    "if 'city' in df.columns:\n",
    "    # Sri Lanka climate zone mapping (simplified)\n",
    "    wet_zone_cities = ['Colombo', 'Galle', 'Ratnapura', 'Kalutara']\n",
    "    dry_zone_cities = ['Anuradhapura', 'Hambantota', 'Jaffna', 'Mannar']\n",
    "    intermediate_zone_cities = ['Kandy', 'Badulla', 'Kurunegala']\n",
    "\n",
    "    # Create climate zone indicators\n",
    "    df['wet_zone'] = df['city'].isin(wet_zone_cities).astype(int)\n",
    "    df['dry_zone'] = df['city'].isin(dry_zone_cities).astype(int)\n",
    "    df['intermediate_zone'] = df['city'].isin(intermediate_zone_cities).astype(int)\n",
    "\n",
    "\n",
    "# Add cyclical encoding for seasonal patterns\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['dayofyear']/365)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['dayofyear']/365)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * 12/24)  # Assume noon for daily data\n",
    "df['hour_cos'] = np.cos(2 * np.pi * 12/24)\n",
    "\n",
    "# Handle time columns: sunrise and sunset\n",
    "def time_to_minutes(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        if isinstance(time_str, str) and ('AM' in time_str or 'PM' in time_str):\n",
    "            time_obj = pd.to_datetime(time_str, format='%I:%M:%S %p').time()\n",
    "        else:\n",
    "            time_obj = pd.to_datetime(time_str).time()\n",
    "        return time_obj.hour * 60 + time_obj.minute\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "if 'sunrise' in df.columns:\n",
    "    df['sunrise_minutes'] = df['sunrise'].apply(time_to_minutes)\n",
    "    df['sunset_minutes'] = df['sunset'].apply(time_to_minutes)\n",
    "    df['daylight_minutes'] = df['sunset_minutes'] - df['sunrise_minutes']\n",
    "\n",
    "# IMPROVEMENT: Enhanced precipitation features\n",
    "# 1. Calculate rolling statistics with more window sizes\n",
    "precip_related_cols = [col for col in df.columns if any(x in col.lower()\n",
    "                      for x in ['precipitation', 'rain', 'humid', 'pressure', 'wind'])]\n",
    "\n",
    "print(f\"\\nPrecipitation-related columns found: {precip_related_cols}\")\n",
    "\n",
    "# More fine-grained window sizes for better pattern detection\n",
    "window_sizes = [3, 5, 7, 10, 14, 21, 30]\n",
    "\n",
    "for col in precip_related_cols:\n",
    "    if col in df.columns and df[col].dtype.kind in 'ifc':  # Numeric columns only\n",
    "        # Calculate rolling statistics\n",
    "        for window in window_sizes:\n",
    "            if len(df) > window:\n",
    "                df[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window=window, min_periods=1).mean()\n",
    "                df[f'{col}_rolling_std_{window}d'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
    "\n",
    "                # Add median and max for better extreme event capture\n",
    "                df[f'{col}_rolling_median_{window}d'] = df[col].rolling(window=window, min_periods=1).median()\n",
    "                df[f'{col}_rolling_max_{window}d'] = df[col].rolling(window=window, min_periods=1).max()\n",
    "\n",
    "        # Calculate day-to-day changes (first derivative)\n",
    "        df[f'{col}_change'] = df[col].diff().fillna(0)\n",
    "\n",
    "        # Calculate change acceleration (second derivative)\n",
    "        df[f'{col}_change_accel'] = df[f'{col}_change'].diff().fillna(0)\n",
    "\n",
    "        # Exponential weighted features - give more weight to recent values\n",
    "        df[f'{col}_ewm_mean'] = df[col].ewm(span=7).mean()\n",
    "        df[f'{col}_ewm_std'] = df[col].ewm(span=7).std()\n",
    "\n",
    "# 2. Add more sophisticated precipitation event features\n",
    "if 'precipitation_sum' in df.columns:\n",
    "    # Define different precipitation thresholds\n",
    "    df['light_rain'] = ((df['precipitation_sum'] > 0.1) & (df['precipitation_sum'] <= 2.5)).astype(int)\n",
    "    df['moderate_rain'] = ((df['precipitation_sum'] > 2.5) & (df['precipitation_sum'] <= 10)).astype(int)\n",
    "    df['heavy_rain'] = ((df['precipitation_sum'] > 10) & (df['precipitation_sum'] <= 50)).astype(int)\n",
    "    df['extreme_rain'] = (df['precipitation_sum'] > 50).astype(int)\n",
    "\n",
    "    # Calculate rain streak features\n",
    "    rain_streak = 0\n",
    "    rain_streaks = []\n",
    "\n",
    "    for rain in df['precipitation_sum'] > 0.5:\n",
    "        if rain:\n",
    "            rain_streak += 1\n",
    "        else:\n",
    "            rain_streak = 0\n",
    "        rain_streaks.append(rain_streak)\n",
    "\n",
    "    df['rain_streak'] = rain_streaks\n",
    "\n",
    "    # Calculate frequency features for different intensities\n",
    "    for window in window_sizes:\n",
    "        if len(df) > window:\n",
    "            df[f'light_rain_freq_{window}d'] = df['light_rain'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'moderate_rain_freq_{window}d'] = df['moderate_rain'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'heavy_rain_freq_{window}d'] = df['heavy_rain'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'extreme_rain_freq_{window}d'] = df['extreme_rain'].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "            # Also calculate total rain days\n",
    "            df[f'rain_days_{window}d'] = (df['precipitation_sum'] > 0.1).rolling(window=window, min_periods=1).sum()\n",
    "\n",
    "# 3. Weather pattern changes with more context\n",
    "if 'weathercode' in df.columns:\n",
    "    df['weather_change'] = (df['weathercode'].diff() != 0).astype(int)\n",
    "\n",
    "    # Get dominant weather pattern in last week\n",
    "    if len(df) > 7:\n",
    "        df['dominant_weathercode_7d'] = df['weathercode'].rolling(window=7).apply(\n",
    "            lambda x: x.value_counts().index[0] if not x.empty else np.nan)\n",
    "\n",
    "        # Is current weather different from dominant pattern\n",
    "        df['weather_anomaly'] = (df['weathercode'] != df['dominant_weathercode_7d']).astype(int)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "cat_cols = ['city', 'country']\n",
    "encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded = encoder.fit_transform(df[[col]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=[f'{col}_{i}' for i in range(encoded.shape[1])])\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "        encoders[col] = encoder\n",
    "        print(f\"Encoded {col} into {encoded.shape[1]} categories\")\n",
    "\n",
    "# Feature selection - drop redundant and non-useful columns\n",
    "base_drop_cols = ['time', 'sunrise', 'sunset'] + cat_cols\n",
    "\n",
    "# Remove temperature_mean if we have max and min (derived feature)\n",
    "if 'temperature_2m_mean' in df.columns and 'temperature_2m_max' in df.columns and 'temperature_2m_min' in df.columns:\n",
    "    base_drop_cols.append('temperature_2m_mean')\n",
    "    print(\"Removing temperature_2m_mean as it's derived from max and min\")\n",
    "\n",
    "# Remove apparent_temperature_mean if we have max and min (derived feature)\n",
    "if 'apparent_temperature_mean' in df.columns and 'apparent_temperature_max' in df.columns and 'apparent_temperature_min' in df.columns:\n",
    "    base_drop_cols.append('apparent_temperature_mean')\n",
    "    print(\"Removing apparent_temperature_mean as it's derived from max and min\")\n",
    "\n",
    "# Remove snow-related features for tropical climate (Sri Lanka)\n",
    "snow_cols = [col for col in df.columns if 'snow' in col.lower()]\n",
    "base_drop_cols.extend(snow_cols)\n",
    "print(f\"Removing {len(snow_cols)} snow-related features as they're irrelevant for Sri Lanka\")\n",
    "\n",
    "# IMPROVEMENT: Try to fetch ENSO data (simplified version)\n",
    "try:\n",
    "    # This is a simplified example - you would need to use a proper API or dataset\n",
    "    # Normally you'd get this from NOAA or similar source\n",
    "    # For this example, we'll create simulated ENSO data\n",
    "\n",
    "    # Generate synthetic ENSO index for the date range in the dataset\n",
    "    start_date = df['time'].min()\n",
    "    end_date = df['time'].max()\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "\n",
    "    # Create a synthetic oscillating ENSO pattern\n",
    "    enso_values = 0.5 * np.sin(np.linspace(0, 4*np.pi, len(date_range)))\n",
    "    enso_df = pd.DataFrame({\n",
    "        'date': date_range,\n",
    "        'enso_index': enso_values\n",
    "    })\n",
    "\n",
    "    # Resample to daily frequency with forward fill\n",
    "    enso_df = enso_df.set_index('date')\n",
    "    daily_enso = enso_df.resample('D').ffill()\n",
    "\n",
    "    # Merge with main dataframe\n",
    "    df_with_date = df.copy()\n",
    "    df_with_date['date'] = df_with_date['time'].dt.date\n",
    "    df_with_date['date'] = pd.to_datetime(df_with_date['date'])\n",
    "\n",
    "    # Merge ENSO data\n",
    "    df_with_enso = pd.merge(df_with_date, daily_enso, left_on='date', right_index=True, how='left')\n",
    "\n",
    "    # Replace the main dataframe if merge was successful\n",
    "    if 'enso_index' in df_with_enso.columns:\n",
    "        df = df_with_enso\n",
    "        print(\"Added ENSO index data\")\n",
    "\n",
    "    # Clean up temporary date column\n",
    "    if 'date' in df.columns:\n",
    "        df = df.drop('date', axis=1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch ENSO data: {e}\")\n",
    "    print(\"Proceeding without ENSO indices\")\n",
    "\n",
    "# Define forecast target(s)\n",
    "target_cols = ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum']\n",
    "\n",
    "# If we transformed precipitation earlier, use the original for evaluation\n",
    "if 'precipitation_sum_original' in df.columns:\n",
    "    target_cols = ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum_original']\n",
    "\n",
    "binary_precip_col = ['light_rain', 'moderate_rain', 'heavy_rain', 'extreme_rain']\n",
    "print(f\"\\nTarget columns: {target_cols}\")\n",
    "print(f\"Binary precipitation targets: {binary_precip_col}\")\n",
    "\n",
    "# Select features\n",
    "feature_cols = [col for col in df.columns if col not in base_drop_cols\n",
    "               and col not in target_cols\n",
    "               and col not in binary_precip_col\n",
    "               and col != 'precipitation_sum_original']  # Exclude original precip if we created it\n",
    "\n",
    "print(f\"\\nSelected feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols[:10], \"... and more\")\n",
    "\n",
    "# Print summary stats for precipitation\n",
    "if 'precipitation_sum_original' in df.columns:\n",
    "    print(\"\\nPrecipitation Summary Statistics:\")\n",
    "    print(df['precipitation_sum_original'].describe())\n",
    "\n",
    "    print(f\"Days with light rain: {df['light_rain'].sum()} ({df['light_rain'].mean()*100:.1f}%)\")\n",
    "    print(f\"Days with moderate rain: {df['moderate_rain'].sum()} ({df['moderate_rain'].mean()*100:.1f}%)\")\n",
    "    print(f\"Days with heavy rain: {df['heavy_rain'].sum()} ({df['heavy_rain'].mean()*100:.1f}%)\")\n",
    "    print(f\"Days with extreme rain: {df['extreme_rain'].sum()} ({df['extreme_rain'].mean()*100:.1f}%)\")\n",
    "\n",
    "# IMPROVEMENT: Set cutoff dates based on monsoon seasons for better model testing\n",
    "# We want the test set to include both SW and NE monsoon periods\n",
    "df = df.sort_values('time')\n",
    "\n",
    "# Use 80-10-10 split but ensure test set has representative monsoon seasons\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:train_size+val_size]\n",
    "test_df = df.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_df)} rows (from {train_df['time'].min()} to {train_df['time'].max()})\")\n",
    "print(f\"Validation set: {len(val_df)} rows (from {val_df['time'].min()} to {val_df['time'].max()})\")\n",
    "print(f\"Test set: {len(test_df)} rows (from {test_df['time'].min()} to {test_df['time'].max()})\")\n",
    "\n",
    "# Check monsoon representation in test set\n",
    "if 'southwest_monsoon' in test_df.columns and 'northeast_monsoon' in test_df.columns:\n",
    "    sw_monsoon_days = test_df['southwest_monsoon'].sum()\n",
    "    ne_monsoon_days = test_df['northeast_monsoon'].sum()\n",
    "    print(f\"Test set includes {sw_monsoon_days} SW monsoon days and {ne_monsoon_days} NE monsoon days\")\n",
    "\n",
    "# IMPROVEMENT: Use more sophisticated scaling techniques\n",
    "# For precipitation, we already applied sinh-arcsinh transformation\n",
    "# For other features, we'll use a robust scaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Use robust scaling for better handling of outliers\n",
    "scaler_features = RobustScaler()  # Less sensitive to outliers than StandardScaler\n",
    "scaler_targets = RobustScaler()   # Better preserves the distribution shape\n",
    "\n",
    "# Fit scalers on training data only\n",
    "train_features = train_df[feature_cols].copy()\n",
    "train_targets = train_df[target_cols].copy()\n",
    "train_binary_target = train_df[binary_precip_col].copy()\n",
    "\n",
    "# Transform all datasets - features\n",
    "train_features_scaled = scaler_features.fit_transform(train_features)\n",
    "val_features_scaled = scaler_features.transform(val_df[feature_cols])\n",
    "test_features_scaled = scaler_features.transform(test_df[feature_cols])\n",
    "\n",
    "# Transform all datasets - targets\n",
    "train_targets_scaled = scaler_targets.fit_transform(train_targets)\n",
    "val_targets_scaled = scaler_targets.transform(val_df[target_cols])\n",
    "test_targets_scaled = scaler_targets.transform(test_df[target_cols])\n",
    "\n",
    "# Binary target doesn't need scaling, but we need to concatenate multiple binary targets\n",
    "train_binary = np.hstack([train_df[col].values.reshape(-1, 1) for col in binary_precip_col])\n",
    "val_binary = np.hstack([val_df[col].values.reshape(-1, 1) for col in binary_precip_col])\n",
    "test_binary = np.hstack([test_df[col].values.reshape(-1, 1) for col in binary_precip_col])\n",
    "\n",
    "# IMPROVEMENT: Longer sequence length (60 days) to better capture seasonal patterns\n",
    "# Create sequences for LSTM with increased sequence length\n",
    "def create_sequences(features, targets, binary_targets=None, seq_length=60, pred_length=5):\n",
    "    \"\"\"Create sequences with optional binary target output\"\"\"\n",
    "    X, y = [], []\n",
    "    binary_y = []\n",
    "\n",
    "    for i in range(len(features) - seq_length - pred_length + 1):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(targets[i+seq_length:i+seq_length+pred_length])\n",
    "\n",
    "        if binary_targets is not None:\n",
    "            binary_y.append(binary_targets[i+seq_length:i+seq_length+pred_length])\n",
    "\n",
    "    if binary_targets is not None:\n",
    "        return np.array(X), np.array(y), np.array(binary_y)\n",
    "    else:\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters - INCREASED SEQUENCE LENGTH for better pattern capture\n",
    "sequence_length = 60  # Increased from 30 to 60 days\n",
    "prediction_length = 5  # Predict 5 days ahead\n",
    "\n",
    "# Create sequences with binary targets\n",
    "X_train, y_train, binary_y_train = create_sequences(\n",
    "    train_features_scaled, train_targets_scaled, train_binary,\n",
    "    seq_length=sequence_length, pred_length=prediction_length)\n",
    "\n",
    "X_val, y_val, binary_y_val = create_sequences(\n",
    "    val_features_scaled, val_targets_scaled, val_binary,\n",
    "    seq_length=sequence_length, pred_length=prediction_length)\n",
    "\n",
    "X_test, y_test, binary_y_test = create_sequences(\n",
    "    test_features_scaled, test_targets_scaled, test_binary,\n",
    "    seq_length=sequence_length, pred_length=prediction_length)\n",
    "\n",
    "print(f\"\\nSequence input shape: {X_train.shape}\")\n",
    "print(f\"Sequence output shape: {y_train.shape}\")\n",
    "print(f\"Binary output shape: {binary_y_train.shape}\")\n",
    "\n",
    "# Flatten target arrays for model training\n",
    "y_train_flat = y_train.reshape(y_train.shape[0], -1)\n",
    "y_val_flat = y_val.reshape(y_val.shape[0], -1)\n",
    "y_test_flat = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "# IMPROVEMENT: Asymmetric loss function that penalizes precipitation underestimation more heavily\n",
    "def asymmetric_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function that penalizes underestimation of precipitation (false negatives)\n",
    "    more heavily than overestimation (false positives).\n",
    "\n",
    "    For precipitation values (every 3rd value starting at index 2), we apply higher\n",
    "    weight when the prediction is lower than the actual value.\n",
    "    \"\"\"\n",
    "    # Calculate squared errors\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "\n",
    "    # Extract precipitation values (every 3rd column starting from index 2)\n",
    "    precip_indices = tf.range(2, tf.shape(y_true)[1], 3)\n",
    "\n",
    "    # Create weights tensor (default weight = 1.0)\n",
    "    weights = tf.ones_like(squared_error)\n",
    "\n",
    "    # For each precipitation column\n",
    "    for i in range(0, prediction_length):\n",
    "        idx = 2 + i * 3  # Index of precipitation column\n",
    "\n",
    "        if idx < tf.shape(y_true)[1]:\n",
    "            # Extract precipitation values\n",
    "            true_vals = y_true[:, idx]\n",
    "            pred_vals = y_pred[:, idx]\n",
    "\n",
    "            # Calculate error direction: positive error means underestimation\n",
    "            error = true_vals - pred_vals\n",
    "\n",
    "            # Create weights: 2.5x for underestimation, 1.0 for overestimation\n",
    "            precip_weights = tf.where(error > 0, 2.5, 1.0)\n",
    "\n",
    "            # Create indices for updating the weights tensor\n",
    "            indices = tf.stack([tf.range(tf.shape(weights)[0]), tf.ones_like(tf.range(tf.shape(weights)[0])) * idx], axis=1)\n",
    "\n",
    "            # Update weights for this precipitation column\n",
    "            weights = tf.tensor_scatter_nd_update(weights, indices, precip_weights)\n",
    "\n",
    "    # Apply weights to squared errors and take mean\n",
    "    weighted_squared_error = squared_error * weights\n",
    "    return tf.reduce_mean(weighted_squared_error)\n",
    "\n",
    "# IMPROVEMENT: Residual ConvLSTM model with attention for better precipitation forecasting\n",
    "def create_advanced_model(input_shape, num_targets, num_days):\n",
    "    \"\"\"\n",
    "    Create an advanced model with:\n",
    "    1. Convolutional layers to extract spatial patterns\n",
    "    2. LSTM layers with residual connections\n",
    "    3. Self-attention mechanism\n",
    "    4. Multi-headed regression\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # 1D CNN layers to extract features\n",
    "    x = Conv1D(64, kernel_size=5, padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Second CNN layer\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Store the CNN output for residual connection\n",
    "    cnn_output = x\n",
    "\n",
    "    # Bidirectional LSTM with increased capacity\n",
    "    x = Bidirectional(LSTM(192, return_sequences=True, activation='tanh'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Self-attention mechanism\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=4, key_dim=48\n",
    "    )(x, x)\n",
    "\n",
    "    # Add residual connection around attention\n",
    "    x = Add()([attention_output, x])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second LSTM layer\n",
    "    x = Bidirectional(LSTM(128, return_sequences=False, activation='tanh'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Residual connection from CNN (need to match dimensions)\n",
    "    # Global average pooling to reduce CNN output dimensions\n",
    "    cnn_pooled = tf.keras.layers.GlobalAveragePooling1D()(cnn_output)\n",
    "    cnn_pooled = Dense(256)(cnn_pooled)  # Match dimensions with LSTM output\n",
    "\n",
    "    # Combine LSTM output with CNN features\n",
    "    x = Add()([x, cnn_pooled])\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Common dense layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Classification outputs for rain intensity categories\n",
    "    rain_outputs = []\n",
    "    for i in range(num_days):\n",
    "        for j, category in enumerate(['light', 'moderate', 'heavy', 'extreme']):\n",
    "            rain_output = Dense(16, activation='relu')(x)\n",
    "            rain_output = Dense(1, activation='sigmoid',\n",
    "                               name=f'day{i+1}_{category}_rain')(rain_output)\n",
    "            rain_outputs.append(rain_output)\n",
    "\n",
    "    # Main regression output\n",
    "    regression_output = Dense(num_targets * num_days, name='main_output')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=rain_outputs + [regression_output])\n",
    "\n",
    "    # Prepare loss dictionary\n",
    "    losses = {'main_output': asymmetric_mse}\n",
    "    loss_weights = {'main_output': 1.0}\n",
    "\n",
    "    # Add binary crossentropy for all rain category outputs\n",
    "    for i in range(len(rain_outputs)):\n",
    "        output_name = model.outputs[i].name.split('/')[0]\n",
    "        losses[output_name] = 'binary_crossentropy'\n",
    "        loss_weights[output_name] = 0.1  # Lower weight for classification tasks\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=losses,\n",
    "        loss_weights=loss_weights,\n",
    "        metrics={'main_output': 'mae'}\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# IMPROVEMENT: Quantile regression model for better extreme event capture\n",
    "def create_quantile_regression_model(input_shape, num_targets, num_days):\n",
    "    \"\"\"Create a model that predicts multiple quantiles (50th, 90th) for precipitation\"\"\"\n",
    "\n",
    "    # Define quantile loss function\n",
    "    def quantile_loss(q, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        return K.mean(K.maximum(q * error, (q - 1) * error))\n",
    "\n",
    "    # Median (q=0.5) quantile loss\n",
    "    q50_loss = lambda y, f: quantile_loss(0.5, y, f)\n",
    "    # Upper (q=0.9) quantile loss for extreme events\n",
    "    q90_loss = lambda y, f: quantile_loss(0.9, y, f)\n",
    "\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Shared layers\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Common dense layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Median (q=0.5) output - standard predictions\n",
    "    median_output = Dense(num_targets * num_days, name='q50_output')(x)\n",
    "\n",
    "    # Upper quantile (q=0.9) output - helps with extreme precipitation\n",
    "    upper_output = Dense(num_targets * num_days, name='q90_output')(x)\n",
    "\n",
    "    # Create model with multiple outputs\n",
    "    model = Model(inputs=inputs, outputs=[median_output, upper_output])\n",
    "\n",
    "    # Compile with appropriate losses\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={'q50_output': q50_loss, 'q90_output': q90_loss},\n",
    "        loss_weights={'q50_output': 1.0, 'q90_output': 0.5},\n",
    "        metrics={'q50_output': 'mae', 'q90_output': 'mae'}\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Standard Enhanced model (updated with higher capacity)\n",
    "def create_enhanced_model(input_shape, output_shape):\n",
    "    \"\"\"Create an enhanced LSTM model with higher capacity\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # First layer: larger capacity (256 units)\n",
    "    model.add(LSTM(256, activation='tanh', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Second layer\n",
    "    model.add(LSTM(192, activation='tanh', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Third layer\n",
    "    model.add(LSTM(128, activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(output_shape))\n",
    "\n",
    "    # Compile with asymmetric loss function\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=asymmetric_mse,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the advanced model\n",
    "def train_advanced_model(X_train, y_train, binary_y_train,\n",
    "                        X_val, y_val, binary_y_val,\n",
    "                        X_test, y_test, binary_y_test):\n",
    "    \"\"\"Train the advanced model\"\"\"\n",
    "\n",
    "    print(\"\\n\\nTraining ADVANCED model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    num_targets = y_train.shape[2]  # temp_max, temp_min, precip\n",
    "    num_days = y_train.shape[1]     # 5 days\n",
    "\n",
    "    # Create model\n",
    "    model = create_advanced_model(input_shape, num_targets, num_days)\n",
    "\n",
    "    # Prepare classification targets - reshape to match output structure\n",
    "    binary_targets_train = []\n",
    "    binary_targets_val = []\n",
    "\n",
    "    # For each prediction day and rain category\n",
    "    for i in range(num_days):\n",
    "        for j in range(binary_y_train.shape[2]):  # 4 categories: light, moderate, heavy, extreme\n",
    "            binary_targets_train.append(binary_y_train[:, i, j])\n",
    "            binary_targets_val.append(binary_y_val[:, i, j])\n",
    "\n",
    "    # Add main regression target\n",
    "    train_targets = binary_targets_train + [y_train_flat]\n",
    "    val_targets = binary_targets_val + [y_val_flat]\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_advanced_weather_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train model with more epochs and reduced batch size\n",
    "    history = model.fit(\n",
    "        X_train, train_targets,\n",
    "        validation_data=(X_val, val_targets),\n",
    "        epochs=100,  # More epochs with early stopping\n",
    "        batch_size=16,  # Smaller batch size for better generalization\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Prepare test targets for evaluation\n",
    "    binary_targets_test = []\n",
    "    for i in range(num_days):\n",
    "        for j in range(binary_y_test.shape[2]):\n",
    "            binary_targets_test.append(binary_y_test[:, i, j])\n",
    "\n",
    "    test_targets = binary_targets_test + [y_test_flat]\n",
    "    test_results = model.evaluate(X_test, test_targets, verbose=0)\n",
    "\n",
    "    # Main output is the last one\n",
    "    main_output_idx = len(model.output_names) - 1\n",
    "    test_loss = test_results[0]  # Overall loss\n",
    "    main_mae = test_results[main_output_idx + 1]  # MAE for main output\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    main_predictions = predictions[-1]  # Last output is main regression\n",
    "\n",
    "    # Reshape to original dimensions\n",
    "    main_predictions_reshaped = main_predictions.reshape(y_test.shape)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Format classification predictions\n",
    "    binary_predictions = []\n",
    "    binary_accuracies = []\n",
    "\n",
    "    # Process each binary output (one per day per rain category)\n",
    "    curr_pred_idx = 0\n",
    "    for i in range(num_days):\n",
    "        day_preds = []\n",
    "        day_accs = []\n",
    "\n",
    "        for j in range(binary_y_test.shape[2]):\n",
    "            pred = (predictions[curr_pred_idx] > 0.5).astype(int)\n",
    "            true = binary_y_test[:, i, j]\n",
    "            acc = accuracy_score(true, pred)\n",
    "\n",
    "            day_preds.append(pred)\n",
    "            day_accs.append(acc)\n",
    "            curr_pred_idx += 1\n",
    "\n",
    "        binary_predictions.append(day_preds)\n",
    "        binary_accuracies.append(day_accs)\n",
    "\n",
    "    # Calculate average accuracy by rain category\n",
    "    rain_categories = ['light', 'moderate', 'heavy', 'extreme']\n",
    "    category_accuracies = {}\n",
    "\n",
    "    for j, category in enumerate(rain_categories):\n",
    "        # Average across all days for this category\n",
    "        category_acc = np.mean([binary_accuracies[i][j] for i in range(num_days)])\n",
    "        category_accuracies[category] = category_acc\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'test_mae': main_mae,\n",
    "        'predictions': main_predictions_reshaped,\n",
    "        'binary_predictions': binary_predictions,\n",
    "        'category_accuracies': category_accuracies,\n",
    "        'avg_accuracy': np.mean(list(category_accuracies.values())),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\nADVANCED Model Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Regression Test MAE: {main_mae:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    print(\"\\nAccuracy by Rain Category:\")\n",
    "    for category, acc in category_accuracies.items():\n",
    "        print(f\"  {category.capitalize()}: {acc:.4f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Train the quantile regression model\n",
    "def train_quantile_model(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train the quantile regression model\"\"\"\n",
    "\n",
    "    print(\"\\n\\nTraining QUANTILE model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    num_targets = y_train.shape[2]\n",
    "    num_days = y_train.shape[1]\n",
    "\n",
    "    # Create model\n",
    "    model = create_quantile_regression_model(input_shape, num_targets, num_days)\n",
    "\n",
    "    # Train model\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_quantile_weather_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, [y_train_flat, y_train_flat],  # Same targets for both outputs during training\n",
    "        validation_data=(X_val, [y_val_flat, y_val_flat]),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    test_results = model.evaluate(X_test, [y_test_flat, y_test_flat], verbose=0)\n",
    "    test_loss = test_results[0]  # Overall loss\n",
    "    q50_mae = test_results[2]    # MAE for median predictions\n",
    "    q90_mae = test_results[4]    # MAE for upper quantile predictions\n",
    "\n",
    "    # Make predictions\n",
    "    q50_pred, q90_pred = model.predict(X_test)\n",
    "\n",
    "    # Reshape predictions to original dimensions\n",
    "    q50_pred_reshaped = q50_pred.reshape(y_test.shape)\n",
    "    q90_pred_reshaped = q90_pred.reshape(y_test.shape)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'q50_mae': q50_mae,\n",
    "        'q90_mae': q90_mae,\n",
    "        'q50_predictions': q50_pred_reshaped,\n",
    "        'q90_predictions': q90_pred_reshaped,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\nQUANTILE Model Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Median (q50) Test MAE: {q50_mae:.4f}\")\n",
    "    print(f\"Upper (q90) Test MAE: {q90_mae:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Train the enhanced baseline model\n",
    "def train_enhanced_baseline(X_train, y_train_flat, X_val, y_val_flat, X_test, y_test):\n",
    "    \"\"\"Train an enhanced baseline model with higher capacity\"\"\"\n",
    "\n",
    "    print(\"\\n\\nTraining ENHANCED BASELINE model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    output_shape = y_train_flat.shape[1]\n",
    "\n",
    "    # Create model\n",
    "    model = create_enhanced_model(input_shape, output_shape)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_enhanced_baseline_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train_flat,\n",
    "        validation_data=(X_val, y_val_flat),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_mae = model.evaluate(X_test, y_test_flat)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_flat = model.predict(X_test)\n",
    "    predictions = predictions_flat.reshape(y_test.shape)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'test_mae': test_mae,\n",
    "        'predictions': predictions,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\nENHANCED BASELINE Model Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Train all models\n",
    "print(\"\\n\\n==== TRAINING SRI LANKA PRECIPITATION MODELS ====\")\n",
    "enhanced_result = train_enhanced_baseline(\n",
    "    X_train, y_train_flat,\n",
    "    X_val, y_val_flat,\n",
    "    X_test, y_test_flat\n",
    ")\n",
    "\n",
    "advanced_result = train_advanced_model(\n",
    "    X_train, y_train, binary_y_train,\n",
    "    X_val, y_val, binary_y_val,\n",
    "    X_test, y_test, binary_y_test\n",
    ")\n",
    "\n",
    "quantile_result = train_quantile_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "# IMPROVEMENT: Create ensemble model that combines predictions from all models\n",
    "# with special focus on high precipitation events\n",
    "def create_advanced_ensemble_predictions(models_dict, y_test):\n",
    "    \"\"\"\n",
    "    Create ensemble predictions by combining all models with intelligent weighting\n",
    "    based on precipitation intensity:\n",
    "    - For low precipitation: Give more weight to median predictions\n",
    "    - For high precipitation: Give more weight to upper quantile and advanced model\n",
    "    \"\"\"\n",
    "    # Extract predictions\n",
    "    enhanced_preds = models_dict['enhanced']['predictions']\n",
    "    advanced_preds = models_dict['advanced']['predictions']\n",
    "    q50_preds = models_dict['quantile']['q50_predictions']\n",
    "    q90_preds = models_dict['quantile']['q90_predictions']\n",
    "\n",
    "    # Create ensemble predictions array\n",
    "    ensemble_predictions = np.zeros_like(y_test)\n",
    "\n",
    "    # For each sample, day, and target\n",
    "    for sample_idx in range(y_test.shape[0]):\n",
    "        for day_idx in range(y_test.shape[1]):\n",
    "            for target_idx in range(y_test.shape[2]):\n",
    "                # Is this a precipitation prediction?\n",
    "                is_precip = (target_idx == 2)\n",
    "\n",
    "                if is_precip:\n",
    "                    # Compute ensemble prediction for precipitation with adaptive weighting\n",
    "\n",
    "                    # Get all precipitation predictions for this point\n",
    "                    enhanced_val = enhanced_preds[sample_idx, day_idx, target_idx]\n",
    "                    advanced_val = advanced_preds[sample_idx, day_idx, target_idx]\n",
    "                    q50_val = q50_preds[sample_idx, day_idx, target_idx]\n",
    "                    q90_val = q90_preds[sample_idx, day_idx, target_idx]\n",
    "\n",
    "                    # Calculate maximum predicted value\n",
    "                    max_pred = max(enhanced_val, advanced_val, q50_val)\n",
    "\n",
    "                    # Use different weighting strategies based on predicted precipitation\n",
    "                    if max_pred > 0.75:  # High precipitation likely (in normalized scale)\n",
    "                        # Give more weight to q90 and advanced model\n",
    "                        weights = {\n",
    "                            'enhanced': 0.1,\n",
    "                            'advanced': 0.4,\n",
    "                            'q50': 0.1,\n",
    "                            'q90': 0.4\n",
    "                        }\n",
    "                    else:  # Low/normal precipitation\n",
    "                        # More balanced weighting\n",
    "                        weights = {\n",
    "                            'enhanced': 0.25,\n",
    "                            'advanced': 0.25,\n",
    "                            'q50': 0.35,\n",
    "                            'q90': 0.15\n",
    "                        }\n",
    "\n",
    "                    # Calculate weighted ensemble\n",
    "                    ensemble_val = (\n",
    "                        weights['enhanced'] * enhanced_val +\n",
    "                        weights['advanced'] * advanced_val +\n",
    "                        weights['q50'] * q50_val +\n",
    "                        weights['q90'] * q90_val\n",
    "                    )\n",
    "\n",
    "                    ensemble_predictions[sample_idx, day_idx, target_idx] = ensemble_val\n",
    "                else:\n",
    "                    # For temperature: Simple average of all models except q90\n",
    "                    ensemble_predictions[sample_idx, day_idx, target_idx] = (\n",
    "                        enhanced_preds[sample_idx, day_idx, target_idx] * 0.35 +\n",
    "                        advanced_preds[sample_idx, day_idx, target_idx] * 0.35 +\n",
    "                        q50_preds[sample_idx, day_idx, target_idx] * 0.3\n",
    "                    )\n",
    "\n",
    "    return ensemble_predictions\n",
    "\n",
    "# Collect all model results\n",
    "models_dict = {\n",
    "    'enhanced': enhanced_result,\n",
    "    'advanced': advanced_result,\n",
    "    'quantile': quantile_result\n",
    "}\n",
    "\n",
    "# Create advanced ensemble predictions\n",
    "ensemble_predictions = create_advanced_ensemble_predictions(models_dict, y_test)\n",
    "\n",
    "# Calculate metrics for all models including ensemble\n",
    "def inverse_transform_predictions(predictions, targets, scaler):\n",
    "    \"\"\"Transform scaled values back to original range\"\"\"\n",
    "    pred_reshaped = predictions.reshape(-1, targets.shape[-1])\n",
    "    targets_reshaped = targets.reshape(-1, targets.shape[-1])\n",
    "\n",
    "    pred_original = scaler.inverse_transform(pred_reshaped)\n",
    "    targets_original = targets_original.reshape(targets.shape)\n",
    "\n",
    "    return pred_original, targets_original\n",
    "\n",
    "# Process all models to get metrics\n",
    "detailed_metrics = {}\n",
    "\n",
    "# Process enhanced baseline\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    enhanced_result['predictions'], y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['enhanced'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Process advanced model\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    advanced_result['predictions'], y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['advanced'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Process quantile model (q50 - median predictions)\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    quantile_result['q50_predictions'], y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['quantile'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Process ensemble model\n",
    "predictions_original, targets_original = inverse_transform_predictions(\n",
    "    ensemble_predictions, y_test, scaler_targets\n",
    ")\n",
    "\n",
    "metrics = {}\n",
    "for i, var in enumerate(target_cols):\n",
    "    metrics[var] = {}\n",
    "    for day in range(prediction_length):\n",
    "        true = targets_original[:, day, i]\n",
    "        pred = predictions_original[:, day, i]\n",
    "        mae = np.mean(np.abs(true - pred))\n",
    "        rmse = np.sqrt(np.mean((true - pred)**2))\n",
    "        metrics[var][f'Day {day+1}'] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "detailed_metrics['ensemble'] = {\n",
    "    'metrics': metrics,\n",
    "    'predictions_original': predictions_original,\n",
    "    'targets_original': targets_original\n",
    "}\n",
    "\n",
    "# Calculate overall model metrics\n",
    "print(\"\\n\\n==== PRECIPITATION FORECAST EVALUATION ====\")\n",
    "print(\"\\nPrecipitation Prediction Performance:\")\n",
    "for model_type, result in detailed_metrics.items():\n",
    "    precip_metrics = result['metrics'][target_cols[2]]  # Use correct target name\n",
    "    avg_mae = np.mean([d['MAE'] for d in precip_metrics.values()])\n",
    "    avg_rmse = np.mean([d['RMSE'] for d in precip_metrics.values()])\n",
    "    print(f\"{model_type.upper()}: Avg MAE = {avg_mae:.2f}, Avg RMSE = {avg_rmse:.2f}\")\n",
    "\n",
    "# Find best model for each target\n",
    "print(\"\\nBest model for each target variable:\")\n",
    "for var_idx, var_name in enumerate(target_cols):\n",
    "    var_maes = {}\n",
    "    for model_type, result in detailed_metrics.items():\n",
    "        avg_mae = np.mean([result['metrics'][var_name][f'Day {day}']['MAE'] for day in range(1, prediction_length + 1)])\n",
    "        var_maes[model_type] = avg_mae\n",
    "\n",
    "    best_model = min(var_maes, key=var_maes.get)\n",
    "    print(f\"{var_name}: {best_model.upper()} (MAE: {var_maes[best_model]:.2f})\")\n",
    "\n",
    "# Plot comparison of precipitation predictions\n",
    "def plot_precipitation_comparison(detailed_metrics, variable_name):\n",
    "    \"\"\"Compare precipitation predictions across models\"\"\"\n",
    "    var_idx = target_cols.index(variable_name)\n",
    "    sample_indices = [0, 1, 2, 3, 4]  # First 5 samples\n",
    "\n",
    "    model_colors = {\n",
    "        'enhanced': 'c--',\n",
    "        'advanced': 'm--',\n",
    "        'quantile': 'g--',\n",
    "        'ensemble': 'y--'\n",
    "    }\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # Get actual values (same for all models)\n",
    "        true_values = list(detailed_metrics.values())[0]['targets_original'][idx, :, var_idx]\n",
    "        plt.plot(range(len(true_values)), true_values, 'bo-', linewidth=2, label='Actual')\n",
    "\n",
    "        # Plot predicted values for each model\n",
    "        for model_type, result in detailed_metrics.items():\n",
    "            pred_values = result['predictions_original'][idx, :, var_idx]\n",
    "            plt.plot(range(len(pred_values)), pred_values, model_colors[model_type],\n",
    "                     linewidth=2, label=f'{model_type.capitalize()}')\n",
    "\n",
    "        plt.title(f'Sri Lanka {variable_name.title()} - Sample {idx+1}')\n",
    "        plt.ylabel(f'{variable_name}')\n",
    "        plt.xlabel('Day')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'sri_lanka_{variable_name}_sample_{idx+1}.png')\n",
    "        plt.show()\n",
    "\n",
    "# Find high precipitation samples\n",
    "def find_high_precip_samples(detailed_metrics, threshold=15):\n",
    "    \"\"\"Find samples with high precipitation values\"\"\"\n",
    "    var_idx = target_cols.index(target_cols[2])  # Use correct target name\n",
    "    targets = list(detailed_metrics.values())[0]['targets_original']\n",
    "\n",
    "    high_precip_indices = []\n",
    "    for i in range(len(targets)):\n",
    "        if np.max(targets[i, :, var_idx]) > threshold:\n",
    "            high_precip_indices.append(i)\n",
    "\n",
    "    return high_precip_indices[:5]  # Return at most 5 samples\n",
    "\n",
    "# Plot precipitation predictions\n",
    "print(\"\\nPlotting precipitation comparison...\")\n",
    "plot_precipitation_comparison(detailed_metrics, target_cols[2])  # Use correct target name\n",
    "\n",
    "# Plot high precipitation events\n",
    "high_precip_indices = find_high_precip_samples(detailed_metrics)\n",
    "print(f\"\\nFound {len(high_precip_indices)} samples with high precipitation\")\n",
    "\n",
    "if high_precip_indices:\n",
    "    for idx in high_precip_indices:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        var_idx = target_cols.index(target_cols[2])  # Use correct target name\n",
    "\n",
    "        # Plot actual values\n",
    "        true_values = list(detailed_metrics.values())[0]['targets_original'][idx, :, var_idx]\n",
    "        plt.plot(range(len(true_values)), true_values, 'bo-', linewidth=2, label='Actual')\n",
    "\n",
    "        # Plot predicted values for each model\n",
    "        model_colors = {\n",
    "            'enhanced': 'c--',\n",
    "            'advanced': 'm--',\n",
    "            'quantile': 'g--',\n",
    "            'ensemble': 'y--'\n",
    "        }\n",
    "\n",
    "        for model_type, result in detailed_metrics.items():\n",
    "            pred_values = result['predictions_original'][idx, :, var_idx]\n",
    "            plt.plot(range(len(pred_values)), pred_values, model_colors[model_type],\n",
    "                     linewidth=2, label=f'{model_type.capitalize()}')\n",
    "\n",
    "        plt.title(f'Sri Lanka High Precipitation Event - Sample {idx}')\n",
    "        plt.ylabel('Precipitation (mm)')\n",
    "        plt.xlabel('Day')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'sri_lanka_high_precipitation_sample_{idx}.png')\n",
    "        plt.show()\n",
    "\n",
    "# Print precipitation performance by day\n",
    "print(\"\\nPrecipitation Prediction Performance by Day:\")\n",
    "for day in range(1, prediction_length + 1):\n",
    "    print(f\"\\nDay {day}:\")\n",
    "    for model_type, result in detailed_metrics.items():\n",
    "        mae = result['metrics'][target_cols[2]][f'Day {day}']['MAE']  # Use correct target name\n",
    "        rmse = result['metrics'][target_cols[2]][f'Day {day}']['RMSE']\n",
    "        print(f\"  {model_type.upper()}: MAE = {mae:.2f}, RMSE = {rmse:.2f}\")\n",
    "\n",
    "# Generate final model comparison summary\n",
    "print(\"\\n\\n===== MODEL COMPARISON SUMMARY =====\")\n",
    "print(\"Overall Metrics:\")\n",
    "\n",
    "# Print details for each model\n",
    "print(f\"ENHANCED: MAE = {enhanced_result['test_mae']:.4f}, Training time: {enhanced_result['training_time']:.2f} seconds\")\n",
    "print(f\"ADVANCED: MAE = {advanced_result['test_mae']:.4f}, Training time: {advanced_result['training_time']:.2f} seconds\")\n",
    "print(f\"QUANTILE (q50): MAE = {quantile_result['q50_mae']:.4f}, Training time: {quantile_result['training_time']:.2f} seconds\")\n",
    "\n",
    "# Calculate ensemble MAE\n",
    "ensemble_mae = np.mean(np.abs(detailed_metrics['ensemble']['predictions_original'] -\n",
    "                            detailed_metrics['ensemble']['targets_original']))\n",
    "print(f\"ENSEMBLE: MAE = {ensemble_mae:.4f}\")\n",
    "\n",
    "# Calculate improvement over previous best model\n",
    "previous_best_mae = 3.45  # From earlier result\n",
    "best_model_type = min(detailed_metrics, key=lambda x: np.mean([d['MAE'] for d in detailed_metrics[x]['metrics'][target_cols[2]].values()]))\n",
    "best_model_mae = np.mean([d['MAE'] for d in detailed_metrics[best_model_type]['metrics'][target_cols[2]].values()])\n",
    "improvement = (previous_best_mae - best_model_mae) / previous_best_mae * 100\n",
    "\n",
    "print(f\"\\nBest model for precipitation: {best_model_type.upper()} with MAE = {best_model_mae:.2f}\")\n",
    "print(f\"Improvement over previous best model: {improvement:.1f}%\")\n",
    "\n",
    "# Save models\n",
    "enhanced_result['model'].save('sri_lanka_enhanced_precipitation_model.h5')\n",
    "print(\"Enhanced model saved as 'sri_lanka_enhanced_precipitation_model.h5'\")\n",
    "\n",
    "advanced_result['model'].save('sri_lanka_advanced_precipitation_model.h5')\n",
    "print(\"Advanced model saved as 'sri_lanka_advanced_precipitation_model.h5'\")\n",
    "\n",
    "quantile_result['model'].save('sri_lanka_quantile_precipitation_model.h5')\n",
    "print(\"Quantile model saved as 'sri_lanka_quantile_precipitation_model.h5'\")\n",
    "\n",
    "print(\"\\nAll model training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
